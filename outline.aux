\relax 
\citation{Gartner}
\citation{fortunearticle}
\citation{xiaofeature}
\citation{bdas}
\citation{alexandrov2014stratosphere}
\citation{rahm2000data}
\citation{kandel2012}
\citation{gokhale2014corleone,park2014crowdfill}
\citation{wang1999sample}
\citation{DBLP:conf/eurosys/AgarwalMPMMS13}
\select@language{USenglish}
\@writefile{toc}{\select@language{USenglish}}
\@writefile{lof}{\select@language{USenglish}}
\@writefile{lot}{\select@language{USenglish}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces ActiveClean\xspace  is an anytime framework for training models on dirty data. Expensive data transformations prior to model training are budgeted with a non-uniform sampling that prioritizes examples that are most likely to change the model. }}{1}}
\newlabel{sys-arch}{{1}{1}}
\citation{fortunearticle}
\citation{wang1999sample}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Setup}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivating Example}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The two naive solutions to the problem: writing-back the cleaned data, and training only on the sample. Writing back to the cleaned data potentially creates unreliable mixtures of data, and training only on the sample creates an issue of dimensionality and sample size. }}{2}}
\newlabel{update-arch1}{{2}{2}}
\citation{wang1999sample}
\citation{bergman2015query}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces ActiveClean\xspace  starts with a dirty model and tries to iterative correct the error in batches. In many datasets, this leads to an improved convergence rate over naive uniform sampling. }}{3}}
\newlabel{sys-arch2}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Preliminaries}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Systematic Biases}{3}}
\newlabel{activeclean}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Architecture}{3}}
\newlabel{arch}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Overview}{3}}
\citation{DBLP:journals/pvldb/YakoutENOI11,DBLP:journals/pvldb/FanLMTY10,khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{bertossi2013data}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{park2014crowdfill}
\citation{gokhale2014corleone,DBLP:journals/pvldb/KopckeTR10}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Examples}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Constraint-based Errors}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Value Filling}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Entity Resolution}{4}}
\citation{nikolic2014linview}
\@writefile{toc}{\contentsline {section}{\numberline {4}Iterative Model Update}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Update Problem}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Exploiting the Model's Geometry}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces In ActiveClean\xspace  , we explore an important class of Machine Learning problems where the loss function (i.e., training error) varies convexly. A stale model can be thought of as a sub-optimal point and we want to move towards the optimal point. }}{5}}
\newlabel{update-arch2}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Iterative Model Correction Algorithm}{5}}
\citation{dekel2012optimal}
\citation{dekel2012optimal,bertsekas2011incremental,zhao2014stochastic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Selecting the Parameters}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Physical Design Considerations}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Stochastic Gradient Descent}{6}}
\newlabel{sgd}{{4.4}{6}}
\newlabel{unbiased}{{1}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Reducing the Variance $\sigma ^2$}{6}}
\newlabel{dist-samp}{{4.4.1}{6}}
\citation{mcbook}
\citation{zhao2014stochastic}
\newlabel{impsample}{{2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The minimum variance estimate of an expected value of a function of a random variable $\mathbb  {E}(f(X))$ is found by placing more samples (marked in yellow) on the ``spikes" of the function.}}{7}}
\newlabel{update-arch3}{{5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Approximating the Optimal Distribution}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Avoiding Regression}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Error Decoupling}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Linear Approximation}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Maintaining Decoupled Averages}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Relaxing the Enumeration Assumption}{9}}
\newlabel{imperfect}{{6}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Why do we partition?}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Using a classifier}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Error Estimate}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Experimental Setup and Notation}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Scenarios}{9}}
\citation{feng2014robust}
\citation{li2014improved}
\citation{wang1999sample}
\citation{guillory2009active}
\citation{gokhale2014corleone}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Alternative Algorithms}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Experiment 1. Effect of Cleaning}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (a) Robust techniques work best when corrupted data are random and look atypical. (b)Data cleaning can provide reliable performance in both the systematically corrupted setting and randomly corrupted setting.}}{10}}
\newlabel{sys-rand}{{6}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Experiment 2. Budgeted Cleaning}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}2a. Alternative Algorithms}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}2b. Source of Improvements}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the true result in comparison to Active Learning and SampleClean. We show the relative model error as a function of the number of examples cleaned. }}{11}}
\newlabel{prio-perf}{{7}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces We clean 10\% of the data with the alternative algorithms and also include variants of ActiveClean\xspace  with optimization removed. We plot the relative error w.r.t the optimized ActiveClean\xspace  . Both partitioning and importance sampling lead to significant reductions in error. }}{11}}
\newlabel{opts}{{8}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}2c. Error Dependence}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces We corrupt an increasing number of entries in the data matrix. At about 30\% corrupted, ActiveClean\xspace  is no longer more efficient than SampleClean. }}{11}}
\newlabel{bias}{{9}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}2d. Testing Accuracy}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Experiment 3. Partioning and Estimation}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}3a. Basic Performance}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the maximum test accuracy in comparison to Active Learning and SampleClean. The reductions in model error correlate well with increased test accuracy in two of the datasets (a) and (b). In (c), we find insignificant gains in terms of test accuracy. }}{12}}
\newlabel{prio-tperf}{{10}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Even with a classifier ActiveClean\xspace  converges faster than Active Learning and SampleClean. We plot the error on a log scale. However, initially ActiveClean\xspace  is comparable in performance to Active Learning, as the classifier acquires training data. }}{12}}
\newlabel{pred-perf}{{11}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}3b. Classifiable Errors}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}3c. Error Estimation}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Errors that are less random are easier to classify, and lead to more significant reductions in relative model error. }}{12}}
\newlabel{tradeoffs2}{{12}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The Taylor series approximation gives more accurate estimates when the amount of cleaned data is small. }}{12}}
\newlabel{tradeoffs3}{{13}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Real Scenarios}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}Replacing Corrupted Data}{12}}
\citation{zhao2014stochastic}
\citation{qu2014randomized}
\citation{rineas2012fast}
\citation{settles2010active}
\citation{settles2010active}
\citation{pan2010survey}
\citation{wainwright2012privacy,duchi2013local}
\citation{nelson2012query}
\citation{altowim2014progressive}
\citation{volkovs2014continuous}
\citation{bergman2015query}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces We experiment with two forms of corruption in the MNIST image datasets: 5x5 block removal and making the images fuzzy. Image (a) shows an uncorrupted ``9", image (b) shows one corrupted with block removal, and image (c) shows one that is corrupted with fuzziness. }}{13}}
\newlabel{mnist-corr}{{14}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Related Work}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces For both types of corruption, ActiveClean\xspace  converges faster than Active Learning and SampleClean. We plot the relative error as function of the number of cleaned examples }}{13}}
\newlabel{mnist}{{15}{13}}
\citation{jaggi2014communication}
\bibstyle{abbrv}
\bibdata{ref}
\bibcite{bdas}{1}
\bibcite{fortunearticle}{2}
\bibcite{DBLP:conf/eurosys/AgarwalMPMMS13}{3}
\bibcite{alexandrov2014stratosphere}{4}
\bibcite{altowim2014progressive}{5}
\bibcite{bergman2015query}{6}
\bibcite{bertossi2013data}{7}
\bibcite{bertsekas2011incremental}{8}
\bibcite{dekel2012optimal}{9}
\bibcite{duchi2013local}{10}
\bibcite{DBLP:journals/pvldb/FanLMTY10}{11}
\bibcite{feng2014robust}{12}
\bibcite{gokhale2014corleone}{13}
\bibcite{guillory2009active}{14}
\bibcite{jaggi2014communication}{15}
\bibcite{kandel2012}{16}
\bibcite{khayyat2015bigdansing}{17}
\bibcite{DBLP:journals/pvldb/KopckeTR10}{18}
\bibcite{li2014improved}{19}
\bibcite{nelson2012query}{20}
\bibcite{nikolic2014linview}{21}
\bibcite{mcbook}{22}
\bibcite{pan2010survey}{23}
\bibcite{park2014crowdfill}{24}
\bibcite{qu2014randomized}{25}
\bibcite{rahm2000data}{26}
\bibcite{settles2010active}{27}
\bibcite{Gartner}{28}
\bibcite{volkovs2014continuous}{29}
\bibcite{wainwright2012privacy}{30}
\bibcite{wang1999sample}{31}
\bibcite{xiaofeature}{32}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {10}References}{14}}
\bibcite{DBLP:journals/pvldb/YakoutENOI11}{33}
\bibcite{zhao2014stochastic}{34}
