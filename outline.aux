\relax 
\citation{Gartner}
\citation{fortunearticle}
\citation{xiaofeature}
\citation{rahm2000data}
\citation{kandel2012}
\citation{gokhale2014corleone,park2014crowdfill}
\citation{wang1999sample}
\citation{DBLP:conf/eurosys/AgarwalMPMMS13}
\select@language{USenglish}
\@writefile{toc}{\select@language{USenglish}}
\@writefile{lof}{\select@language{USenglish}}
\@writefile{lot}{\select@language{USenglish}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces ActiveClean\xspace  is an anytime framework for training models on dirty data. Expensive data transformations prior to model training are budgeted with a non-uniform sampling that prioritizes examples that are most likely to change the model. }}{1}}
\newlabel{sys-arch}{{1}{1}}
\citation{bottou2012stochastic}
\citation{drineas2012fast,settles2010active}
\citation{settles2010active}
\citation{fortunearticle}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Setup}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivating Example}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine Learning and Loss Minimization}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Systematic Biases}{2}}
\citation{wang1999sample}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces ActiveClean\xspace  starts with a dirty model and tries to iterative correct the error in batches. In many datasets, this leads to an improved convergence rate over naive uniform sampling. }}{3}}
\newlabel{sys-arch2}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Data Cleaning on a Budget}{3}}
\newlabel{activeclean}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Two perspectives on error}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Architecture}{3}}
\newlabel{arch}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Overview}{3}}
\citation{DBLP:journals/pvldb/YakoutENOI11,DBLP:journals/pvldb/FanLMTY10,khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{bertossi2013data}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{park2014crowdfill}
\citation{gokhale2014corleone,DBLP:journals/pvldb/KopckeTR10}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}User Specified Steps}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Error Detection}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Error Repair}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Examples}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Constraint-based Errors}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Value Filling}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Entity Resolution}{4}}
\citation{nikolic2014linview}
\@writefile{toc}{\contentsline {section}{\numberline {4}Iterative Model Update}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Update Problem}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Naive Solutions}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Exploiting the Model's Geometry}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The two naive solutions to the problem: writing-back the cleaned data, and training only on the sample. Writing back to the cleaned data potentially creates unreliable mixtures of data, and training only on the sample creates an issue of dimensionality and sample size. }}{5}}
\newlabel{update-arch1}{{3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces In ActiveClean\xspace  , we explore an important class of Machine Learning problems where the loss function (i.e., training error) varies convexly. A stale model can be thought of as a sub-optimal point and we want to move towards the optimal point. }}{5}}
\newlabel{update-arch2}{{4}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Selecting the Parameters}{5}}
\citation{dekel2012optimal}
\citation{dekel2012optimal,bertsekas2011incremental,zhao2014stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Stochastic Gradient Descent}{6}}
\newlabel{unbiased}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Reducing the Variance $\sigma ^2$}{6}}
\newlabel{dist-samp}{{4.4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Summary}{6}}
\citation{wang1999sample}
\citation{settles2010active}
\@writefile{toc}{\contentsline {section}{\numberline {5}Approximating the Optimal Distribution}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Error Decoupling}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Maintaining Decoupled Averages}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Algorithm}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Physical Design Considerations}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Comparative Analysis}{7}}
\newlabel{analysis}{{5.5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Imperfect Partitioning}{8}}
\newlabel{imperfect}{{6}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Why do we partition?}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Revised Formulation}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Revised Algorithm}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Experimental Setup and Notation}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Scenarios}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Experiment 1. Effect of Cleaning}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Experiment 2. Prioritization}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}2a. Alternative Algorithms}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Robust techniques work best when corrupted data are random and look atypical. Data cleaning can provide reliable performance in both the systematically corrupted setting and randomly corrupted setting.}}{9}}
\newlabel{sys-rand}{{5}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}2b. Source of Improvements}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the true result in comparison to Active Learning and SampleClean. }}{10}}
\newlabel{prio-perf}{{6}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces We clean 10\% of the data with the alternative algorithms and also include variants of ActiveClean\xspace  with optimization removed. We plot the relative error w.r.t the optimized ActiveClean\xspace  . Both partitioning and importance sampling lead to significant reductions in error. }}{10}}
\newlabel{opts}{{7}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces As we increase the biasing nature of the corruption, Active Learning is increasingly erroneous w.r.t ActiveClean\xspace  . }}{10}}
\newlabel{albias}{{8}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}2c. Error Dependence}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces We corrupt an increasing number of entries in the data matrix. At about 30\% corrupted, ActiveClean\xspace  is no longer more efficient than SampleClean. }}{10}}
\newlabel{bias}{{9}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}2d. Testing Accuracy}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces While initially ActiveClean\xspace  is comparable in performance to Active Learning, as the classifier improves, ActiveClean\xspace  converges faster than Active Learning and SampleClean. }}{11}}
\newlabel{pred-perf}{{11}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Experiment 3. Predicates vs. Classification}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}3a. Basic Performance}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}3b. Classifiable Errors}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Experiment 4. Price of a Scan}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Errors that are less random are easier to classify, and lead to more significant reductions in relative model error. }}{11}}
\newlabel{tradeoffs2}{{12}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Real Scenarios}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}Replacing Corrupted Data}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces We experiment with two forms of corruption in the MNIST image datasets: 5x5 block removal and making the images fuzzy. }}{11}}
\newlabel{mnist-corr}{{13}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Related Work}{11}}
\citation{zhao2014stochastic}
\citation{qu2014randomized}
\citation{rineas2012fast}
\citation{settles2010active}
\citation{settles2010active}
\citation{pan2010survey}
\citation{wainwright2012privacy,duchi2013local}
\citation{nelson2012query}
\citation{altowim2014progressive}
\citation{volkovs2014continuous}
\citation{bergman2015query}
\citation{jaggi2014communication}
\bibstyle{abbrv}
\bibstyle{abbrv}
\bibdata{ref}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the maximum test accuracy in comparison to Active Learning and SampleClean. }}{12}}
\newlabel{prio-tperf}{{10}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces For both types of corruption, ActiveClean\xspace  converges faster than Active Learning and SampleClean. }}{12}}
\newlabel{mnist}{{14}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{12}}
\bibcite{fortunearticle}{1}
\bibcite{DBLP:conf/eurosys/AgarwalMPMMS13}{2}
\bibcite{altowim2014progressive}{3}
\bibcite{bergman2015query}{4}
\bibcite{bertossi2013data}{5}
\bibcite{bertsekas2011incremental}{6}
\bibcite{drineas2012fast}{7}
\bibcite{duchi2013local}{8}
\bibcite{DBLP:journals/pvldb/FanLMTY10}{9}
\bibcite{gokhale2014corleone}{10}
\bibcite{jaggi2014communication}{11}
\bibcite{kandel2012}{12}
\bibcite{khayyat2015bigdansing}{13}
\bibcite{DBLP:journals/pvldb/KopckeTR10}{14}
\bibcite{nelson2012query}{15}
\bibcite{nikolic2014linview}{16}
\bibcite{pan2010survey}{17}
\bibcite{park2014crowdfill}{18}
\bibcite{qu2014randomized}{19}
\bibcite{rahm2000data}{20}
\bibcite{settles2010active}{21}
\bibcite{Gartner}{22}
\bibcite{volkovs2014continuous}{23}
\bibcite{wainwright2012privacy}{24}
\bibcite{wang1999sample}{25}
\bibcite{xiaofeature}{26}
\bibcite{DBLP:journals/pvldb/YakoutENOI11}{27}
\bibcite{zhao2014stochastic}{28}
\@writefile{toc}{\contentsline {section}{\numberline {10}References}{13}}
