\relax 
\citation{Gartner}
\citation{fortunearticle}
\citation{xiaofeature}
\citation{rahm2000data}
\citation{kandel2012}
\citation{gokhale2014corleone,park2014crowdfill}
\citation{wang1999sample}
\citation{DBLP:conf/eurosys/AgarwalMPMMS13}
\select@language{USenglish}
\@writefile{toc}{\select@language{USenglish}}
\@writefile{lof}{\select@language{USenglish}}
\@writefile{lot}{\select@language{USenglish}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces ActiveClean\xspace  is an anytime framework for training models on dirty data. Expensive data transformations prior to model training are budgeted with a non-uniform sampling that prioritizes examples that are most likely to change the model. }}{1}}
\newlabel{sys-arch}{{1}{1}}
\citation{bottou2012stochastic}
\citation{drineas2012fast,settles2010active}
\citation{settles2010active}
\citation{fortunearticle}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Setup}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivating Example}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine Learning and Loss Minimization}{2}}
\citation{wang1999sample}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces ActiveClean\xspace  starts with a dirty model and tries to iterative correct the error in batches. In many datasets, this leads to an improved convergence rate over naive uniform sampling. }}{3}}
\newlabel{sys-arch2}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Systematic Biases}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Data Cleaning on a Budget}{3}}
\newlabel{activeclean}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Two perspectives on error}{3}}
\citation{DBLP:journals/pvldb/YakoutENOI11,DBLP:journals/pvldb/FanLMTY10,khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{bertossi2013data}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{park2014crowdfill}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Architecture}{4}}
\newlabel{arch}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Overview}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}User Specified Steps}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Error Detection}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Error Repair}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Examples}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Constraint-based Errors}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Value Filling}{4}}
\citation{gokhale2014corleone,DBLP:journals/pvldb/KopckeTR10}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Entity Resolution}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Iterative Model Update}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Update Problem}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Naive Solutions}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The two naive solutions to the problem: writing-back the cleaned data, and training only on the sample. Writing back to the cleaned data potentially creates unreliable mixtures of data, and training only on the sample creates an issue of dimensionality and sample size. }}{5}}
\newlabel{update-arch1}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Exploiting the Model's Geometry}{5}}
\citation{nikolic2014linview}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces In ActiveClean\xspace  , we explore an important class of Machine Learning problems where the loss function (i.e., training error) varies convexly. A stale model can be thought of as a sub-optimal point and we want to move towards the optimal point. }}{6}}
\newlabel{update-arch2}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Iterative Model Correction Algorithm}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Selecting the Parameters}{6}}
\citation{dekel2012optimal}
\citation{dekel2012optimal,bertsekas2011incremental,zhao2014stochastic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Physical Design Considerations}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Stochastic Gradient Descent}{7}}
\newlabel{sgd}{{4.4}{7}}
\newlabel{unbiased}{{1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Reducing the Variance $\sigma ^2$}{7}}
\newlabel{dist-samp}{{4.4.1}{7}}
\citation{mcbook}
\citation{zhao2014stochastic}
\newlabel{impsample}{{2}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The minimum variance estimate of an expected value of a function of a random variable $\mathbb  {E}(f(X))$ is found by placing more samples (marked in yellow) on the ``spikes" of the function.}}{8}}
\newlabel{update-arch3}{{5}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Approximating the Optimal Distribution}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Avoiding Regression}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Error Decoupling}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Linear Approximation}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Maintaining Decoupled Averages}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Relaxing the Enumeration Assumption}{10}}
\newlabel{imperfect}{{6}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Why do we partition?}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Using a classifier}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Error Estimate}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Experimental Setup and Notation}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Scenarios}{10}}
\citation{feng2014robust}
\citation{li2014improved}
\citation{wang1999sample}
\citation{guillory2009active}
\citation{gokhale2014corleone}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Alternative Algorithms}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Experiment 1. Effect of Cleaning}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (a) Robust techniques work best when corrupted data are random and look atypical. (b)Data cleaning can provide reliable performance in both the systematically corrupted setting and randomly corrupted setting.}}{11}}
\newlabel{sys-rand}{{6}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Experiment 2. Prioritization}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}2a. Alternative Algorithms}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}2b. Source of Improvements}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces We clean 10\% of the data with the alternative algorithms and also include variants of ActiveClean\xspace  with optimization removed. We plot the relative error w.r.t the optimized ActiveClean\xspace  . Both partitioning and importance sampling lead to significant reductions in error. }}{12}}
\newlabel{opts}{{8}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}2c. Error Dependence}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces As we increase the biasing nature of the corruption, Active Learning is increasingly erroneous w.r.t ActiveClean\xspace  . }}{12}}
\newlabel{albias}{{9}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces We corrupt an increasing number of entries in the data matrix. At about 30\% corrupted, ActiveClean\xspace  is no longer more efficient than SampleClean. }}{12}}
\newlabel{bias}{{10}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}2d. Testing Accuracy}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the true result in comparison to Active Learning and SampleClean. We show the relative model error (on a log scale) as a function of the number of examples cleaned. }}{13}}
\newlabel{prio-perf}{{7}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Experiment 3. Predicates vs. Classification}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}3a. Basic Performance}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}3b. Classifiable Errors}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Even with a classifier ActiveClean\xspace  converges faster than Active Learning and SampleClean. We plot the error on a log scale. However, initially ActiveClean\xspace  is comparable in performance to Active Learning, as the classifier acquires training data. }}{13}}
\newlabel{pred-perf}{{12}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Errors that are less random are easier to classify, and lead to more significant reductions in relative model error. }}{13}}
\newlabel{tradeoffs2}{{13}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Experiment 4. Price of a Scan}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Real Scenarios}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}Replacing Corrupted Data}{13}}
\citation{zhao2014stochastic}
\citation{qu2014randomized}
\citation{rineas2012fast}
\citation{settles2010active}
\citation{settles2010active}
\citation{pan2010survey}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the maximum test accuracy in comparison to Active Learning and SampleClean. The reductions in model error correlate well with increased test accuracy in two of the datasets (a) and (b). In (c), we find insignificant gains in terms of test accuracy. }}{14}}
\newlabel{prio-tperf}{{11}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Related Work}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces We experiment with two forms of corruption in the MNIST image datasets: 5x5 block removal and making the images fuzzy. Image (a) shows an uncorrupted ``9", image (b) shows one corrupted with block removal, and image (c) shows one that is corrupted with fuzziness. }}{14}}
\newlabel{mnist-corr}{{14}{14}}
\citation{wainwright2012privacy,duchi2013local}
\citation{nelson2012query}
\citation{altowim2014progressive}
\citation{volkovs2014continuous}
\citation{bergman2015query}
\citation{jaggi2014communication}
\bibstyle{abbrv}
\bibdata{ref}
\bibcite{fortunearticle}{1}
\bibcite{DBLP:conf/eurosys/AgarwalMPMMS13}{2}
\bibcite{altowim2014progressive}{3}
\bibcite{bergman2015query}{4}
\bibcite{bertossi2013data}{5}
\bibcite{bertsekas2011incremental}{6}
\bibcite{dekel2012optimal}{7}
\bibcite{drineas2012fast}{8}
\bibcite{duchi2013local}{9}
\bibcite{DBLP:journals/pvldb/FanLMTY10}{10}
\bibcite{feng2014robust}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces For both types of corruption, ActiveClean\xspace  converges faster than Active Learning and SampleClean. }}{15}}
\newlabel{mnist}{{15}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {10}References}{15}}
\bibcite{gokhale2014corleone}{12}
\bibcite{guillory2009active}{13}
\bibcite{jaggi2014communication}{14}
\bibcite{kandel2012}{15}
\bibcite{khayyat2015bigdansing}{16}
\bibcite{DBLP:journals/pvldb/KopckeTR10}{17}
\bibcite{li2014improved}{18}
\bibcite{nelson2012query}{19}
\bibcite{nikolic2014linview}{20}
\bibcite{mcbook}{21}
\bibcite{pan2010survey}{22}
\bibcite{park2014crowdfill}{23}
\bibcite{qu2014randomized}{24}
\bibcite{rahm2000data}{25}
\bibcite{settles2010active}{26}
\bibcite{Gartner}{27}
\bibcite{volkovs2014continuous}{28}
\bibcite{wainwright2012privacy}{29}
\bibcite{wang1999sample}{30}
\bibcite{xiaofeature}{31}
\bibcite{DBLP:journals/pvldb/YakoutENOI11}{32}
\bibcite{zhao2014stochastic}{33}
