\relax 
\citation{Gartner}
\citation{fortunearticle}
\citation{xiaofeature}
\citation{rahm2000data}
\citation{kandel2012}
\citation{gokhale2014corleone,park2014crowdfill}
\citation{wang1999sample}
\citation{DBLP:conf/eurosys/AgarwalMPMMS13}
\select@language{USenglish}
\@writefile{toc}{\select@language{USenglish}}
\@writefile{lof}{\select@language{USenglish}}
\@writefile{lot}{\select@language{USenglish}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces ActiveClean\xspace  is an anytime framework from training models on dirty data. Expensive data transformations prior to model training are budgeted with a non-uniform sampling that prioritizes examples that are most likely to change the model. }}{1}}
\newlabel{sys-arch}{{1}{1}}
\citation{bottou2012stochastic}
\citation{drineas2012fast,settles2010active}
\citation{settles2010active}
\citation{fortunearticle}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Setup}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivating Example}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine Learning and Loss Minimization}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Systematic Biases}{2}}
\citation{wang1999sample}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces ActiveClean\xspace  starts with a dirty model and tries to iterative correct the error in batches. In many datasets, this leads to an improved convergence rate over naive uniform sampling. }}{3}}
\newlabel{sys-arch2}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Data Cleaning on a Budget}{3}}
\newlabel{activeclean}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Two perspectives on error}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Architecture}{3}}
\newlabel{arch}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Formulation}{3}}
\citation{DBLP:journals/pvldb/YakoutENOI11,DBLP:journals/pvldb/FanLMTY10,khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{bertossi2013data}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{park2014crowdfill}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Error Detection}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Error Sampling}{4}}
\newlabel{imp-samp}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Error Repair}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Incremental Model Update}{4}}
\newlabel{imp-update}{{3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Error Impact Estimate}{4}}
\newlabel{imp-est}{{4}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6}Summary}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Detection and Repair Models}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Constraint-based Errors}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Value Filling}{4}}
\citation{gokhale2014corleone,DBLP:journals/pvldb/KopckeTR10}
\citation{bertsekas2011incremental}
\citation{nikolic2014linview}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Entity Resolution}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Sampling and Model Update}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Stochastic Gradient Descent}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sampling Distribution}{5}}
\newlabel{dist-samp}{{4.2}{5}}
\citation{zhao2014stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Summary}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Approximating the Optimal Distribution}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Error Decoupling}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Maintaining Decoupled Averages}{6}}
\citation{wang1999sample}
\citation{settles2010active}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Algorithm}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Physical Design Considerations}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Comparative Analysis}{7}}
\newlabel{analysis}{{5.5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Imperfect Partitioning}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Why do we partition?}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Revised Formulation}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Revised Algorithm}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Experimental Setup and Notation}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Scenarios}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Experiment 1. Effect of Cleaning}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Robust techniques work best when corrupted data are random and look atypical. Data cleaning can provide reliable performance in both the systematically corrupted setting and randomly corrupted setting.}}{8}}
\newlabel{sys-rand}{{3}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Experiment 2. Prioritization}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}2a. Alternative Algorithms}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}2b. Source of Improvements}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}2c. Error Dependence}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces We clean 10\% of the data with the alternative algorithms and also include variants of ActiveClean\xspace  with optimization removed. We plot the relative error w.r.t the optimized ActiveClean\xspace  . Both partitioning and importance sampling lead to significant reductions in error. }}{9}}
\newlabel{opts}{{5}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces As we increase the biasing nature of the corruption, Active Learning is increasingly erroneous w.r.t ActiveClean\xspace  . }}{9}}
\newlabel{albias}{{6}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}2d. Testing Accuracy}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the true result in comparison to Active Learning and SampleClean. }}{10}}
\newlabel{prio-perf}{{4}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces We corrupt an increasing number of entries in the data matrix. At about 30\% corrupted, ActiveClean\xspace  is no longer more efficient than SampleClean. }}{10}}
\newlabel{bias}{{7}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Experiment 3. Predicates vs. Classification}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}3a. Basic Performance}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}3b. Classifiable Errors}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces While initially ActiveClean\xspace  is comparable in performance to Active Learning, as the classifier improves, ActiveClean\xspace  converges faster than Active Learning and SampleClean. }}{10}}
\newlabel{pred-perf}{{9}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Errors that are less random are easier to classify, and lead to more significant reductions in relative model error. }}{10}}
\newlabel{tradeoffs2}{{10}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Experiment 4. Price of a Scan}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Real Scenarios}{10}}
\citation{zhao2014stochastic}
\citation{qu2014randomized}
\citation{rineas2012fast}
\citation{settles2010active}
\citation{settles2010active}
\citation{pan2010survey}
\citation{wainwright2012privacy,duchi2013local}
\citation{nelson2012query}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the maximum test accuracy in comparison to Active Learning and SampleClean. }}{11}}
\newlabel{prio-tperf}{{8}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}Replacing Corrupted Data}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces We experiment with two forms of corruption in the MNIST image datasets: 5x5 block removal and making the images fuzzy. }}{11}}
\newlabel{mnist-corr}{{11}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Related Work}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces For both types of corruption, ActiveClean\xspace  converges faster than Active Learning and SampleClean. }}{11}}
\newlabel{mnist}{{12}{11}}
\citation{altowim2014progressive}
\citation{volkovs2014continuous}
\citation{bergman2015query}
\citation{jaggi2014communication}
\bibstyle{abbrv}
\bibstyle{abbrv}
\bibdata{ref}
\bibcite{fortunearticle}{1}
\bibcite{DBLP:conf/eurosys/AgarwalMPMMS13}{2}
\bibcite{altowim2014progressive}{3}
\bibcite{bergman2015query}{4}
\bibcite{bertossi2013data}{5}
\bibcite{bertsekas2011incremental}{6}
\bibcite{drineas2012fast}{7}
\bibcite{duchi2013local}{8}
\bibcite{DBLP:journals/pvldb/FanLMTY10}{9}
\bibcite{gokhale2014corleone}{10}
\bibcite{jaggi2014communication}{11}
\bibcite{kandel2012}{12}
\bibcite{khayyat2015bigdansing}{13}
\bibcite{DBLP:journals/pvldb/KopckeTR10}{14}
\bibcite{nelson2012query}{15}
\bibcite{nikolic2014linview}{16}
\bibcite{pan2010survey}{17}
\bibcite{park2014crowdfill}{18}
\bibcite{qu2014randomized}{19}
\bibcite{rahm2000data}{20}
\bibcite{settles2010active}{21}
\bibcite{Gartner}{22}
\bibcite{volkovs2014continuous}{23}
\bibcite{wainwright2012privacy}{24}
\bibcite{wang1999sample}{25}
\bibcite{xiaofeature}{26}
\bibcite{DBLP:journals/pvldb/YakoutENOI11}{27}
\bibcite{zhao2014stochastic}{28}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {10}References}{12}}
