\relax 
\citation{Gartner}
\citation{fortunearticle}
\citation{xiaofeature}
\citation{rahm2000data}
\citation{kandel2012}
\citation{gokhale2014corleone,park2014crowdfill}
\citation{wang1999sample}
\citation{DBLP:conf/eurosys/AgarwalMPMMS13}
\citation{bottou2012stochastic}
\select@language{USenglish}
\@writefile{toc}{\select@language{USenglish}}
\@writefile{lof}{\select@language{USenglish}}
\@writefile{lot}{\select@language{USenglish}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces ActiveClean\xspace  (ActiveClean\xspace  ) is an anytime framework from training models on dirty data. Expensive data transformations prior to model training are budgeted with a non-uniform sampling that prioritizes examples that are most likely to change the model. }}{1}}
\newlabel{sys-arch}{{1}{1}}
\citation{drineas2012fast,settles2010active}
\citation{settles2010active}
\citation{fortunearticle}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivating Example}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine Learning and Loss Minimization}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Systematic Biases}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Two perspectives on error}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Architecture}{3}}
\newlabel{arch}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Formulation}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Error Detection}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Error Sampling}{3}}
\newlabel{imp-samp}{{1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Error Repair}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Incremental Model Update}{3}}
\newlabel{imp-update}{{2}{3}}
\citation{DBLP:journals/pvldb/YakoutENOI11,DBLP:journals/pvldb/FanLMTY10,khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{bertossi2013data}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{park2014crowdfill}
\citation{gokhale2014corleone,DBLP:journals/pvldb/KopckeTR10}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Error Impact Estimate}{4}}
\newlabel{imp-est}{{3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6}Summary}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Detection and Repair Models}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Constraint-based Errors}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Value Filling}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Entity Resolution}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Sampling and Model Update}{4}}
\citation{bertsekas2011incremental}
\citation{nikolic2014linview}
\citation{zhao2014stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Stochastic Gradient Descent}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sampling Distribution}{5}}
\newlabel{dist-samp}{{4.2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Summary}{5}}
\citation{wang1999sample}
\citation{settles2010active}
\@writefile{toc}{\contentsline {section}{\numberline {5}Approximating the Optimal Distribution}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Error Decoupling}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Maintaining Decoupled Averages}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Algorithm}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Physical Design Considerations}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Comparative Analysis}{6}}
\newlabel{analysis}{{5.5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Imperfect Partitioning}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Why do we partition?}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Revised Formulation}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Revised Algorithm}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Experimental Setup and Notation}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Scenarios}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Experiment 1. Effect of Cleaning}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Experiment 2. Prioritization}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Robust techniques work best when corrupted data are random and look atypical. Data cleaning can provide reliable performance in both the systematically corrupted setting and randomly corrupted setting.}}{8}}
\newlabel{sys-rand}{{2}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}2a. Alternative Algorithms}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}2b. Source of Improvements}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the true result in comparison to Active Learning and SampleClean. }}{9}}
\newlabel{prio-perf}{{3}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces We clean 10\% of the data with the alternative algorithms and also include variants of ActiveClean\xspace  with optimization removed. We plot the relative error w.r.t the optimized ActiveClean\xspace  . Both partitioning and importance sampling lead to significant reductions in error. }}{9}}
\newlabel{opts}{{4}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}2c. Error Dependence}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces As we increase the biasing nature of the corruption, Active Learning is increasingly erroneous w.r.t ActiveClean\xspace  . }}{9}}
\newlabel{albias}{{5}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces We corrupt an increasing number of entries in the data matrix. At about 30\% corrupted, ActiveClean\xspace  is no longer more efficient than SampleClean. }}{9}}
\newlabel{bias}{{6}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces While initially ActiveClean\xspace  is comparable in performance to Active Learning, as the classifier improves, ActiveClean\xspace  converges faster than Active Learning and SampleClean. }}{10}}
\newlabel{pred-perf}{{8}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}2d. Testing Accuracy}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Experiment 3. Predicates vs. Classification}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}3a. Basic Performance}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}3b. Classifiable Errors}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Errors that are less random are easier to classify, and lead to more significant reductions in relative model error. }}{10}}
\newlabel{tradeoffs2}{{9}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Experiment 4. Price of a Scan}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Real Scenarios}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}Replacing Corrupted Data}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces We experiment with two forms of corruption in the MNIST image datasets: 5x5 block removal and making the images fuzzy. }}{10}}
\newlabel{mnist-corr}{{10}{10}}
\citation{zhao2014stochastic}
\citation{qu2014randomized}
\citation{rineas2012fast}
\citation{settles2010active}
\citation{settles2010active}
\citation{pan2010survey}
\citation{wainwright2012privacy,duchi2013local}
\citation{nelson2012query}
\citation{altowim2014progressive}
\citation{volkovs2014continuous}
\citation{bergman2015query}
\citation{jaggi2014communication}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the maximum test accuracy in comparison to Active Learning and SampleClean. }}{11}}
\newlabel{prio-tperf}{{7}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces For both types of corruption, ActiveClean\xspace  converges faster than Active Learning and SampleClean. }}{11}}
\newlabel{mnist}{{11}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Related Work}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{11}}
\bibstyle{abbrv}
\bibstyle{abbrv}
\bibdata{ref}
\bibcite{fortunearticle}{1}
\bibcite{DBLP:conf/eurosys/AgarwalMPMMS13}{2}
\bibcite{altowim2014progressive}{3}
\bibcite{bergman2015query}{4}
\bibcite{bertossi2013data}{5}
\bibcite{bertsekas2011incremental}{6}
\bibcite{drineas2012fast}{7}
\bibcite{duchi2013local}{8}
\bibcite{DBLP:journals/pvldb/FanLMTY10}{9}
\bibcite{gokhale2014corleone}{10}
\bibcite{jaggi2014communication}{11}
\bibcite{kandel2012}{12}
\bibcite{khayyat2015bigdansing}{13}
\bibcite{DBLP:journals/pvldb/KopckeTR10}{14}
\bibcite{nelson2012query}{15}
\bibcite{nikolic2014linview}{16}
\bibcite{pan2010survey}{17}
\bibcite{park2014crowdfill}{18}
\bibcite{qu2014randomized}{19}
\bibcite{rahm2000data}{20}
\bibcite{settles2010active}{21}
\bibcite{Gartner}{22}
\bibcite{volkovs2014continuous}{23}
\bibcite{wainwright2012privacy}{24}
\bibcite{wang1999sample}{25}
\bibcite{xiaofeature}{26}
\bibcite{DBLP:journals/pvldb/YakoutENOI11}{27}
\bibcite{zhao2014stochastic}{28}
\@writefile{toc}{\contentsline {section}{\numberline {10}References}{12}}
