\relax 
\citation{Gartner}
\citation{fortunearticle}
\citation{xiaofeature}
\citation{rahm2000data}
\citation{kandel2012}
\citation{gokhale2014corleone,park2014crowdfill}
\citation{wang1999sample}
\citation{DBLP:conf/eurosys/AgarwalMPMMS13}
\select@language{USenglish}
\@writefile{toc}{\select@language{USenglish}}
\@writefile{lof}{\select@language{USenglish}}
\@writefile{lot}{\select@language{USenglish}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces ActiveClean\xspace  is an anytime framework for training models on dirty data. Expensive data transformations prior to model training are budgeted with a non-uniform sampling that prioritizes examples that are most likely to change the model. }}{1}}
\newlabel{sys-arch}{{1}{1}}
\citation{bottou2012stochastic}
\citation{drineas2012fast,settles2010active}
\citation{settles2010active}
\citation{fortunearticle}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Setup}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivating Example}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine Learning and Loss Minimization}{2}}
\citation{wang1999sample}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces ActiveClean\xspace  starts with a dirty model and tries to iterative correct the error in batches. In many datasets, this leads to an improved convergence rate over naive uniform sampling. }}{3}}
\newlabel{sys-arch2}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Systematic Biases}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Data Cleaning on a Budget}{3}}
\newlabel{activeclean}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Two perspectives on error}{3}}
\citation{DBLP:journals/pvldb/YakoutENOI11,DBLP:journals/pvldb/FanLMTY10,khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{bertossi2013data}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{khayyat2015bigdansing}
\citation{DBLP:journals/pvldb/FanLMTY10}
\citation{DBLP:journals/pvldb/YakoutENOI11}
\citation{park2014crowdfill}
\@writefile{toc}{\contentsline {section}{\numberline {3}System Architecture}{4}}
\newlabel{arch}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Overview}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}User Specified Steps}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Error Detection}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Error Repair}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Examples}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Constraint-based Errors}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Value Filling}{4}}
\citation{gokhale2014corleone,DBLP:journals/pvldb/KopckeTR10}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Entity Resolution}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Iterative Model Update}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Update Problem}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Naive Solutions}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The two naive solutions to the problem: writing-back the cleaned data, and training only on the sample. Writing back to the cleaned data potentially creates unreliable mixtures of data, and training only on the sample creates an issue of dimensionality and sample size. }}{5}}
\newlabel{update-arch1}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Exploiting the Model's Geometry}{5}}
\citation{nikolic2014linview}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces In ActiveClean\xspace  , we explore an important class of Machine Learning problems where the loss function (i.e., training error) varies convexly. A stale model can be thought of as a sub-optimal point and we want to move towards the optimal point. }}{6}}
\newlabel{update-arch2}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Iterative Model Correction Algorithm}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Selecting the Parameters}{6}}
\citation{dekel2012optimal}
\citation{dekel2012optimal,bertsekas2011incremental,zhao2014stochastic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Physical Design Considerations}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Stochastic Gradient Descent}{7}}
\newlabel{sgd}{{4.4}{7}}
\newlabel{unbiased}{{1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Reducing the Variance $\sigma ^2$}{7}}
\newlabel{dist-samp}{{4.4.1}{7}}
\citation{mcbook}
\citation{zhao2014stochastic}
\newlabel{impsample}{{2}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The minimum variance estimate of an expected value of a function of a random variable $\mathbb  {E}(f(X))$ is found by placing more samples (marked in yellow) on the ``spikes" of the function.}}{8}}
\newlabel{update-arch3}{{5}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Approximating the Optimal Distribution}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Avoiding Regression}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Error Decoupling}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Linear Approximation}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Maintaining Decoupled Averages}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Relaxing the Enumeration Assumption}{10}}
\newlabel{imperfect}{{6}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Why do we partition?}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Using a classifier}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Error Estimate}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Experimental Setup and Notation}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Scenarios}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Experiment 1. Effect of Cleaning}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Robust techniques work best when corrupted data are random and look atypical. Data cleaning can provide reliable performance in both the systematically corrupted setting and randomly corrupted setting.}}{11}}
\newlabel{sys-rand}{{6}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Experiment 2. Prioritization}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}2a. Alternative Algorithms}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}2b. Source of Improvements}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the true result in comparison to Active Learning and SampleClean. }}{12}}
\newlabel{prio-perf}{{7}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces We clean 10\% of the data with the alternative algorithms and also include variants of ActiveClean\xspace  with optimization removed. We plot the relative error w.r.t the optimized ActiveClean\xspace  . Both partitioning and importance sampling lead to significant reductions in error. }}{12}}
\newlabel{opts}{{8}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}2c. Error Dependence}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces As we increase the biasing nature of the corruption, Active Learning is increasingly erroneous w.r.t ActiveClean\xspace  . }}{12}}
\newlabel{albias}{{9}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces We corrupt an increasing number of entries in the data matrix. At about 30\% corrupted, ActiveClean\xspace  is no longer more efficient than SampleClean. }}{12}}
\newlabel{bias}{{10}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces While initially ActiveClean\xspace  is comparable in performance to Active Learning, as the classifier improves, ActiveClean\xspace  converges faster than Active Learning and SampleClean. }}{13}}
\newlabel{pred-perf}{{12}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}2d. Testing Accuracy}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Experiment 3. Predicates vs. Classification}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}3a. Basic Performance}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}3b. Classifiable Errors}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Errors that are less random are easier to classify, and lead to more significant reductions in relative model error. }}{13}}
\newlabel{tradeoffs2}{{13}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Experiment 4. Price of a Scan}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Real Scenarios}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}Replacing Corrupted Data}{13}}
\citation{zhao2014stochastic}
\citation{qu2014randomized}
\citation{rineas2012fast}
\citation{settles2010active}
\citation{settles2010active}
\citation{pan2010survey}
\citation{wainwright2012privacy,duchi2013local}
\citation{nelson2012query}
\citation{altowim2014progressive}
\citation{volkovs2014continuous}
\citation{bergman2015query}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces ActiveClean\xspace  converges with a smaller sample size to the maximum test accuracy in comparison to Active Learning and SampleClean. }}{14}}
\newlabel{prio-tperf}{{11}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces We experiment with two forms of corruption in the MNIST image datasets: 5x5 block removal and making the images fuzzy. }}{14}}
\newlabel{mnist-corr}{{14}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces For both types of corruption, ActiveClean\xspace  converges faster than Active Learning and SampleClean. }}{14}}
\newlabel{mnist}{{15}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Related Work}{14}}
\citation{jaggi2014communication}
\bibstyle{abbrv}
\bibstyle{abbrv}
\bibdata{ref}
\bibcite{fortunearticle}{1}
\bibcite{DBLP:conf/eurosys/AgarwalMPMMS13}{2}
\bibcite{altowim2014progressive}{3}
\bibcite{bergman2015query}{4}
\bibcite{bertossi2013data}{5}
\bibcite{bertsekas2011incremental}{6}
\bibcite{dekel2012optimal}{7}
\bibcite{drineas2012fast}{8}
\bibcite{duchi2013local}{9}
\bibcite{DBLP:journals/pvldb/FanLMTY10}{10}
\bibcite{gokhale2014corleone}{11}
\bibcite{jaggi2014communication}{12}
\bibcite{kandel2012}{13}
\bibcite{khayyat2015bigdansing}{14}
\bibcite{DBLP:journals/pvldb/KopckeTR10}{15}
\bibcite{nelson2012query}{16}
\bibcite{nikolic2014linview}{17}
\bibcite{mcbook}{18}
\bibcite{pan2010survey}{19}
\bibcite{park2014crowdfill}{20}
\bibcite{qu2014randomized}{21}
\bibcite{rahm2000data}{22}
\bibcite{settles2010active}{23}
\bibcite{Gartner}{24}
\bibcite{volkovs2014continuous}{25}
\bibcite{wainwright2012privacy}{26}
\bibcite{wang1999sample}{27}
\bibcite{xiaofeature}{28}
\bibcite{DBLP:journals/pvldb/YakoutENOI11}{29}
\bibcite{zhao2014stochastic}{30}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {10}References}{15}}
