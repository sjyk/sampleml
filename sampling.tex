\section{Efficiency With Sampling}\label{dist-samp}
The \emph{updater} received a sample with probabilities $p(\cdot)$.
For any distribution where  $p(\cdot) > 0$, we can preserve correctness.
\sys uses a sampling algorithm that selects the most valuable records to clean with higher probability. 

\subsection{Oracle Sampling Problem}
Recall that the convergence rate of an SGD algorithm is bounded by $\sigma^2$ which is the variance of the gradient.
Intuitively, the variance measures how accurately the gradient is estimated from a uniform sample.
Other sampling distributions, while preserving the sample expected value, may have a lower variance.
Thus, the oracle sampling problem is defined as a search over sampling distributions to find the minimum variance sampling distribution.

It can be shown that the optimal distribution over records in $R_{dirty}$ is probabilities proportional to:
\[
p_i \propto \|\nabla\phi(x^{(c)}_i,y^{(c)}_i,\theta^{(t)})\|
\]
The problem is that the optimal distribution leads to a chicken-and-egg problem:
the optimal sampling distribution requires knowing $(x^{(c)}_i,y^{(c)}_i)$, which are the clean values, however we are sampling the values so that they can be cleaned..

\subsection{Dirty Gradient Solution}\label{dgsample}
Since such an oracle does not exist, and one solution is to use the gradient w.r.t the dirty data:
\[
p_i \propto \|\nabla\phi(x^{(d)}_i,y^{(d)}_i,\theta^{(t)})\|
\]
It turns out that the solution works reasonably well in practice on our experimental datasets and has been studied in Machine Learning as the Expected Gradient Length heuristic \cite{settles2010active}.
The contribution in our work is integrating this heuristic with statistically correct updates.
However, intuitively, approximating the oracle as closely as possible can result in improved prioritization.
The subsequent section describes two components, the detector and estimator, that can be used to improve the convergence rate.
Our experiments suggest up-to a 2x improvement in convergence when using these optional optimizations (Section \ref{comp}).