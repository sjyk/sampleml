\section{Background and Problem Setup}\label{background}
This section formalizes the iterative data cleaning and training process and highlights an example application.

\subsection{Statistical Modeling}
In this paper, we will use the term \emph{statistical modeling} to describe a well studied class of analytics problems; ones that can be expressed as the minimization of convex loss functions.
Examples include generalized linear models (including linear and logistic regression), support vector machines, and in fact, means and medians are also special cases. 
Generally, such a formulation is restricted to supervised Machine Learning.

Suppose $x$ is a feature vector and $y$ is a label.
For labeled training examples $\{(x_{i},y_{i})\}_{i=1}^{N}$, the problem is to find a vector of \emph{model parameters} $\theta$ by minimizing a loss function $\phi$ (a function that measures prediction error) over all training examples:
\[
 \theta^{*}=\arg\min_{\theta}\sum_{i=1}^{N}\phi(x_{i},y_{i},\theta)
\]
Where $\phi$ is a convex function in $\theta$.
For example, in a linear regression $\phi$ is:
\[
\phi(x_{i},y_{i},\theta) = \|\theta^Tx_{i} - y_i \|_2^2
\]
Sometimes, a \emph{regularization} term $r(\theta)$ is added to the loss, however, this will not affect our results, and we ignore this term without loss of generality.

\subsection{Data Cleaning}
We assume that there is a one-to-one known mapping between records in a relation $R$ and labeled training examples $(x_{i},y_{i})$.
\sys supports two types of data cleaning operations attribute value transformations and removal.We represent the data cleaning operation as user-defined function $Clean(\cdot)$ which can be applied to a record $r$ to recover a unique clean record $r' = Clean(r)$ with the same schema or remove the record $\emptyset = Clean(r)$.
$Clean(\cdot)$ can be implemented software or represent a manual action by the analyst.
We define the clean relation as a relation of all of the records after cleaning:
\[R_{clean} = \cup_i^N Clean(r_i \in R)\]
Therefore, for every $r' \in R_{clean}$ there exists a unique $r \in R$ in the dirty data.
We acknowledge that this definition is limited as it does \emph{not} cover errors that simultaneously affect multiple records such as record duplication or structure such as schema transformation.
However, there are a number of important examples of supported cleaning operations include, resolving common inconsistencies (e.g., merging ``U.S.A" and ``United States"), filtering outliers (e.g., removing records with values $>1e6$), and standardizing attribute semantics (e.g., ``1.2 miles" and ``1.93 km").
These errors are very common in the context of statistical models on sensors, logs, and human-input datasets. 

The record-by-record cleaning model is not a fundamental restriction of \sys, and our technical report discusses a generalization called the ``set of records" cleaning model.
In this generalization, the $Clean(\cdot)$ function is composed of schema-preserving \textsf{map} and \textsf{filter} applied to the entire dataset.
This can model problems such batch resolution of inconsistencies such as a find-and-replace.
However, mathematically, this does not make a significant difference in the rest of the paper.

\subsection{Use Case: Dollars for Docs \cite{dollarsfordocs}}\label{s:usecase}
ProPublica collected a dataset of corporate donations to doctors to analyze conflicts of interest. 
They reported that some doctors received over \$500,000 in travel, meals, and consultation expenses \cite{dollarsfordocsa}.
ProPublica laboriously curated and cleaned a dataset from the Centers for Medicare and Medicaid Services that listed nearly 250,000 research donations, and aggregated these donations by physician, drug, and pharmaceutical company.
We collected the raw unaggregated data and explored whether suspect donations could be predicted with a model.
This problem is typical of analysis scenarios based on observational data seen in finance, insurance, medicine, and investigative journalism.
The dataset has the following schema:
\begin{lstlisting}[mathescape,basicstyle={\scriptsize}]
Contribution(pi_specialty$\textrm{,}$ drug_name$\textrm{,}$ device_name$\textrm{,}$
corporation$\textrm{,}$ amount$\textrm{,}$ dispute$\textrm{,}$ status)
\end{lstlisting}

\noindent\texttt{pi\_specialty} is a textual attribute describing the specialty of the doctor receiving the donation.

\noindent\texttt{drug\_name} is the branded name of the drug in the research study (null if not a drug).

\noindent\texttt{device\_name} is the branded name of the device in the study (null if not a device).

\noindent\texttt{corporation} is the name of the pharmaceutical providing the donation.

\noindent\texttt{amount} is a numerical attribute representing the donation amount.

\noindent\texttt{dispute} is a Boolean attribute describing whether the research was disputed.

\noindent\texttt{status} is a string label describing whether the  donation was allowed under the declared research protocol. The goal is to predict disallowed  donation. 

\vspace{0.5em}

However, this dataset is very dirty, and the systematic nature of the data corruption can result in an inaccurate model.
On the ProPublica website \cite{dollarsfordocs}, they list numerous types of data problems that had to be cleaned before publishing the data (see Appendix \ref{dfd-errors}).
For example, the most significant donations were made by large companies whose names were also more often inconsistently represented in the data, e.g., ``Pfizer Inc.", ``Pfizer Incorporated", ``Pfizer".
In such scenarios, the effect of systematic error can be serious.
Duplicate representations could artificially reduce the correlation between these entities and suspected contributions.
There were nearly 40,000 of the 250,000 records that had either naming inconsistencies or other inconsistencies in labeling the allowed or disallowed \texttt{status}.
Without data cleaning, the detection rate using a Support Vector Machine was 66\%.
Applying the data cleaning to the entire dataset improved this rate to 97\% in the clean data (Section \ref{dfd-exp}).

\subsection{Problem: Iteration in Model Construction}
Let us consider an analyst designing an SVM classification model on the Dollars For Docs dataset.
When she develops her model on the dirty data, she will find that the detection rate (true positives predicted) is quite low 66\%.
To investigate why, she might examine those records that are labled as flagged but not predicted by the classifier.
She will then discover that there are numerous examples where two records are nearly identical but one is predicted correctly and one is incorrect, and their only difference is an inconsistent representation of one of the attributes.
She will merge those two similar attributes, re-train the model, and repeat this process.
This iterative process can be described as the following pseudocode loop:
\begin{enumerate}[leftmargin=1em]\scriptsize\sloppy
  \item \texttt{Init(iter)}
  \item \texttt{current\_model = Train(R)}
  \item For each t in $\{1,...,iter\}$
  \begin{enumerate}
    \item \texttt{dirty\_sample $=$ Identify(R,current\_model)}
    \item \texttt{clean\_sample $=$ Clean(dirty\_sample)}
    \item \texttt{current\_model $=$ Update(clean\_sample,current\_model, R)}
  \end{enumerate}
  \item \texttt{Output: current\_model}
  \end{enumerate}
The problem is that a naive implementation of \emph{Identify()} and \emph{Update()} can result in incorrect and divergent model training.

\vspace{0.5em}
\textbf{Correctness: } Let us revist Figure \ref{update-arch1} from the introduction. Let us assume that the analyst has implemented an $Identify(\cdot)$ function that returns $k$ candidate dirty records.
The straight-forward application of data cleaning is to repair the corruption in place, and re-train the model after each repair.
Suppose $k \ll N$ records are cleaned, but all of the remaining dirty records are retained in the dataset.

Aggregates over mixtures of different populations of data can result in spurious relationships due to the well-known phenomenon called Simpson's paradox \cite{simpson1951interpretation}.
Simpson's paradox is by no means a corner case, and it has affected the validity of a number of high-profile studies~\cite{simpsonsparadox}; even in the simple case of taking an average over a dataset.
Statistical models are high-dimensional generalizations of these aggregates without closed form techniques to compensate for these biases.
Thus, training models on a mixture of dirty and clean data can lead to unreliable results, where artificial trends introduced by the mixture can be confused for the effects of data cleaning.

An alternative is to avoid the dirty data altogether instead of mixing the two populations, and the model re-training is restricted to only data that are known to be clean.
This approach is similar to SampleClean \cite{wang1999sample}, which was proposed to approximate the results of aggregate queries by applying them to a clean sample of data.
However, high-dimensional models are highly sensitive to sample size.
Figure \ref{update-arch1}c illustrates that, even in two dimensions, models trained from small samples can be as incorrect as the mixing solution described before.

\vspace{0.5em} 

\textbf{Efficiency: } Conversely, hypothetically assume that the analyst has implemented a correct $Update(\cdot)$ primitive and implements $Identify(\cdot)$ with a technique such as Active Learning to select records to clean~\cite{yakout2013don,DBLP:journals/pvldb/YakoutENOI11,gokhale2014corleone}.
Active learning is a technique to carefully select the set of examples to learn the most accurate model.
However, these selection criteria are designed for data distributions that do not change, an assumption which is not true in this setting.
As more data are cleaned, the data distributions and the models change.
Data which may look unimportant in the dirty data might be very valuable to clean in reality, and thus any prioritization has to predict a record's value with respect to an anticipated clean model.



