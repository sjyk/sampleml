\section{Background and Problem Setup}\label{background}
This section formalizes the iterative data cleaning and training process and highlights an example application.

\subsection{Statistical Modeling}
In this paper, we will use the term \emph{statistical modeling} to describe a well studied class of analytics problems; ones that can be expressed as the minimization of convex loss functions.
This class is restricted to supervised Machine Learning.
Examples include linear models (including linear and logistic regression), support vector machines, and in fact, means and medians are also special cases. 

Formally, suppose $x$ is a feature vector and $y$ is a label.
For labeled training examples $\{(x_{i},y_{i})\}_{i=1}^{N}$, the problem is to find a vector of \emph{model parameters} $\theta$ by minimizing a loss function $\phi$ (a function that measures prediction error) over all training examples:
\[
 \theta^{*}=\arg\min_{\theta}\sum_{i=1}^{N}\phi(x_{i},y_{i},\theta)
\]
Where $\phi$ is a convex function in $\theta$.
For example, in a linear regression $\phi$ is:
\[
\phi(x_{i},y_{i},\theta) = \|\theta^Tx_{i} - y_i \|_2^2
\]
Sometimes, a \emph{regularization} term $r(\theta)$ is added to the loss, however, this will not affect our results, and we ignore this term without loss of generality.

\subsection{Data Cleaning}
We assume that there is a one-to-one known mapping between records in a relation $R$ and labeled training examples $(x_{i},y_{i})$.
%\sys supports two types of data cleaning operations attribute value transformations and removal.
We represent the data cleaning operation as general user-defined function $Clean(\cdot)$ that can be applied to a record $r$ to perform two actions: recover a unique clean record $r' = Clean(r)$ with the same schema or remove the record $\emptyset = Clean(r)$.
$Clean(\cdot)$ can be implemented with software or represent a manual action by the analyst.
We define the clean relation as a relation of all of the records after cleaning:
\[R_{clean} = \cup_i^N Clean(r_i \in R)\]
Therefore, for every $r' \in R_{clean}$ there exists a unique $r \in R$ in the dirty data.
We acknowledge that this definition is limited as it does \emph{not} cover errors that simultaneously affect multiple records such as record duplication or structure such as schema transformation.
However, the supported cleaning operations include many important operations, resolving common inconsistencies (e.g., merging ``U.S.A" and ``United States"), filtering outliers (e.g., removing records with values $>1e6$), and standardizing attribute semantics (e.g., ``1.2 miles" and ``1.93 km").
These errors are very common in the context of statistical models on sensors, logs, and human-input datasets. 

The record-by-record cleaning model is not a fundamental restriction of \sys, and our technical report discusses a generalization called the ``set of records" cleaning model~\cite{activecleanarxiv}.
In this generalization, the $Clean(\cdot)$ function is composed of schema-preserving \textsf{map} and \textsf{filter} applied to the entire dataset.
This can model problems such batch resolution of inconsistencies such as a find-and-replace.

\subsection{Use Case: Dollars for Docs}\label{s:usecase}
ProPublica collected a dataset of corporate donations to doctors to analyze conflicts of interest~\cite{dollarsfordocsa}. 
They reported that some doctors received over \$500,000 in travel, meals, and consultation expenses.
In order to make such strong claims about the doctors, ProPublica laboriously curated and cleaned a dataset from the Centers for Medicare and Medicaid Services that listed nearly 250,000 research donations, and aggregated these donations by physician, drug, and pharmaceutical company.
We collected the raw unaggregated data and explored whether suspect donations could be predicted with a model.
This problem is typical of analysis scenarios based on observational data seen in finance, insurance, medicine, and investigative journalism.
The dataset has the following schema:
\begin{lstlisting}[mathescape,basicstyle={\small}]
Contribution(pi_specialty$\textrm{,}$ drug_name$\textrm{,}$ device_name$\textrm{,}$
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$corporation$\textrm{,}$ amount$\textrm{,}$ dispute$\textrm{,}$ status)
\end{lstlisting}

\noindent\texttt{pi\_specialty} is a textual attribute describing the specialty of the doctor receiving the donation.

\noindent\texttt{drug\_name} is the branded name of the drug in the research study (null if not a drug).

\noindent\texttt{device\_name} is the branded name of the device in the study (null if not a device).

\noindent\texttt{corporation} is the name of the pharmaceutical providing the donation.

\noindent\texttt{amount} is a numerical attribute representing the donation amount.

\noindent\texttt{dispute} is a Boolean attribute describing whether the research was disputed.

\noindent\texttt{status} is a string label describing whether the  donation was allowed under the declared research protocol. The goal is to predict disallowed  donation. 

\vspace{0.5em}

However, this dataset is very dirty, and the systematic nature of the data corruption can result in an inaccurate model.
On the ProPublica website \cite{dollarsfordocs}, they list numerous types of data problems that had to be cleaned before publishing the data (see Technical Report~\cite{activecleanarxiv}).
For example, the most significant donations were made by large companies whose names were also more often inconsistently represented in the data, e.g., Pfizer Inc., Pfizer Incorporated, Pfizer.
The data cleaning problem is to resolve this common inconsistency to a canonical name.
Duplicate representations could artificially reduce the correlation between these entities and suspected contributions.
There were nearly 40,000 of the 250,000 records that had either naming inconsistencies or other inconsistencies in labeling the allowed or disallowed \texttt{status}.
Without data cleaning, the detection rate using a Support Vector Machine was 66\%.
Applying the data cleaning to the entire dataset improved this rate to 97\% in the clean data (Section \ref{dfd-exp}).

\subsection{Problem: Iteration in Model Construction}
Let us consider an analyst designing an SVM classification model on the Dollars For Docs dataset.
When she develops her model on the dirty data, she will find that the detection rate (true positives predicted) is quite low 66\%.
To investigate why, she might examine those records that are labled as flagged but not predicted by the classifier.
She will then discover that there are numerous examples where two records are nearly identical but one is predicted correctly and one is incorrect, and their only difference is the \texttt{corporation} attribute: Pfizer and Pfizer Incorporated.
She will merge all records those two similar attributes, re-train the model, and repeat this process.
This iterative process can be described as the following pseudocode loop:
\begin{enumerate}[leftmargin=1em]\scriptsize\sloppy
  \item \texttt{Init(iter)}
  \item \texttt{current\_model = Train(R)}
  \item For each t in $\{1,...,iter\}$
  \begin{enumerate}
    \item \texttt{dirty\_sample $=$ Identify(R,current\_model)}
    \item \texttt{clean\_sample $=$ Clean(dirty\_sample)}
    \item \texttt{current\_model $=$ Update(clean\_sample,current\_model, R)}
  \end{enumerate}
  \item \texttt{Output: current\_model}
  \end{enumerate}

A naive implementation of $Update(\cdot)$ and $Identify(\cdot)$ can result in incorrect and divergent model training.

\vspace{0.5em}
\noindent \textbf{Correctness: } Let us revist Figure \ref{update-arch1} from the introduction. Let us assume that the analyst has implemented an $Identify(\cdot)$ function that returns $k$ candidate dirty records.
The straight-forward application of data cleaning is to repair the corruption in place, and re-train the model after each repair.
Suppose $k \ll N$ records are cleaned, but all of the remaining dirty records are retained in the dataset.
Aggregates over mixtures of different populations of data can result in spurious relationships due to the well-known phenomenon called Simpson's paradox \cite{simpson1951interpretation}.
Simpson's paradox is by no means a corner case, and it has affected the validity of a number of high-profile studies~\cite{simpsonsparadox}.
Thus, training models on a mixture of dirty and clean data can lead to unreliable results, where artificial trends introduced by the mixture can be confused for the effects of data cleaning.

An alternative is to avoid the dirty data altogether instead of mixing the two populations, and the model re-training is restricted to only data that are known to be clean.
This approach is similar to SampleClean \cite{wang1999sample}, which was proposed to approximate the results of aggregate queries by applying them to a clean sample of data.
However, high-dimensional models are highly sensitive to sample size.
Figure \ref{update-arch1}c illustrates that, even in two dimensions, models trained from small samples can be as incorrect as the mixing solution described before.

\vspace{0.5em} 

\noindent \textbf{Efficiency: } Conversely, let us hypothetically assume that the analyst has implemented a correct $Update(\cdot)$ primitive, and now we will discuss the problems in the $Identify(\cdot)$ primitive.
One of the most popular prioritization techniques is Active Learning, which has been widely applied in the context of data cleaning~\cite{DBLP:journals/pvldb/YakoutENOI11,gokhale2014corleone}.
Active Learning considers the problem of selecting the most informative unlabeled examples to label in partially labeled static data, and the key problem in using Active Learning for data cleaning is to formulate the human input as a binary labeling task.
Active Learning iteratively queries new examples to label and integrates those examples into the model.
In contrast, \sys studies a broader problem of accounting for modifications to both features and labels in existing examples.
We have access to dirty values may give us valuable information about which data are likely to be dirty, and which data are likely to impact the model. 
This would be like Active Learning, where instead of unlabled data there are inaccurate prior estimates for the labels.
\sys reduces to a form of Expected Gradient Length Active Learning when there is no structure in the dirty data to exploit (such as completely missing attribute values).




