\section{Importance Sampling and SGD}
In this section, we will describe the main algorithmic contribution of this paper.
We will start by introducing the anytime sampling problem as Stochastic Gradient Descent problem.
We will start by presenting the intuition in an artifical oracular setting, then will complicate the
story by discussing what we have to do to make this practical.

\subsection{Basic Algorithm: Oracular Case}
Mini-batch stochastic gradient descent is an algorithm for finding the optimal value
of $\theta$, given the convex loss $\phi$, and data.
At each iteration, we draw a subset of data at random and update with the average gradient of that subset of data.
 \[
 \theta^{(t+1)}\leftarrow\theta^{(t)}-\gamma\sum_{i\in S^{(t)}}\nabla\phi(x_{i},y_{i},\theta^{(t)})
 \]
$\gamma$ is a hyperparameter which is called the learning rate.
There is extensive literature in machine learning for choosing $\gamma$ appropriately, and in this work, we will assume that it is set by the system.

We treat the anytime sampling problem as a stochastic gradient descent problem.
We start with an initialization which is the dirty model, the model trained completely on the dirty data.
Then, we sample a mini-batch of data on which we apply our expensive data cleaning operations.
We stop when the budget is reached.
This basic algorithm has performance lower bound of:
\[
\mathbb{E}(\|\theta^{(t)}-\theta^{*}\|^{2})\le O(\frac{\sigma^{2}}{bt})
\]
and for the non-strongly convex case \reminder{explain better}
\[
\mathbb{E}(\|\theta^{(t)}-\theta^{*}\|^{2})\le O(\frac{\sigma^{2}}{\sqrt{bt}})
\]
where $\mathbb{E}(\|\nabla\phi\|_{2}^{2})=\sigma^{2}$.

In the data cleaning setting, where the data cleaning is often far more expensive than calculating a gradient for each point (e.g., Entity Resolution), we can consider non-uniform sampling techniques that require some pre-processing of the data.
If we choose our sampling technique carefuly, we can reduce the $\sigma^{2}$ term in the error bound.
The technique that we will apply is called importance sampling.
Importance sampling is a technique that allows us to draw a batch from a non-uniform distribution but reweight the result to give us an unbiased estimate of the gradient.
The gradient update becomes:
\[\theta^{(t+1)}\leftarrow\theta^{(t)}-\gamma\sum_{i\in S^{(t)}}\frac{1}{p_{i}}\nabla\phi(x_{i},y_{i},\theta^{(t)})\]

It is established \cite{zhao2014stochastic} that the optimal sampling distribution (lowest variance) is:
\[p_{i}\propto\|\nabla\phi(x_{i},y_{i},\theta^{(t)})\|\]
In the standard Machine Learning setting, it is not practical to use this optimal sampling distribution.
Determining this sampling distribution requires calculate the gradient at every data point which defeats the purpose of stochastic optimization. 
However, in our setting, we are actually calculating the gradient of the cleaned data:
\[\phi(C(x_{i}),C(y_{i}),\theta^{(t)})\]

Part of the motivation of this work is that the function $C$ is often expensive to evaluate.
Instead, suppose we had an estimate of $\hat{C}$ that was cheap to evaluate.
We could then apply this estimate to every point and get an approximately optimal sampling distribution:
\[p_{i}\propto\|\nabla\phi(\hat{C}(x_{i}),\hat{C}(y_{i}),\theta^{(t)})\|\]
This builds the inutition for the basic algorithm that we will propose.
As we clean data, we learn an estimator $\hat{C}$ which can be used to calculate the approximately optimal
sampling distribution.

\begin{enumerate}[noitemsep]
\item Initialize with $\theta^{(0)}$ as the dirty model.
\item For rounds i=1...T
\begin{enumerate}
	\item Update $\hat{C}$
	\item Sample $\frac{B}{T}$ data points with probabilites as described above
	\item Apply data cleaning to the sample of data, or if already cleaned skip
	\item Calculate the average gradient of the sample
	\item Apply weighted gradient descent
\end{enumerate}
\item Return $\theta^{(T)}$
\end{enumerate} 

We have been purposefully vague about how to learn the estimator $\hat{C}$ to simplify the intuition
and analysis of the algorithm.
In the subsequent subsections, we will describe different settings and how we can efficiently learn $\hat{C}$.
Essentially, all variants of the basic algorithm will modify step 2 in differing ways.

\subsubsection{Oracular Case: Analysis}
Suppose, our estimator $\hat{C}$ is perfect where $\hat{C} = C$ for all input data.
In this case, we can show that the error bound for the importance sampled algorithm is strictly lower
than for the non-importance sampled algorithm.

\begin{theorem}
The importance sampled algorithm converges to the same solution as the uniformly sampled algorithm.
Furthermore, $\sigma_{imp} \le \sigma_{uni}$.
The inequality is tight only when all the data is identical.
\end{theorem}
\begin{proof}
The importance sampling gives an unbiased estimate of the gradient at each iteration so it converges to
the same solution.
\reminder{Probably cite TR for proof}
\end{proof}

We know that learning a perfect estimator for $C$ is equivalent to cleaning all of the data and 
is not feasible.
We next analyze how inaccurate $\hat{C}$ and still preserve the gains of importance sampling.
\begin{theorem}
Let the importance sampling distribution be perturbed by $\epsilon_i$ at each data point: 
\[\|p_i \propto \nabla\phi(\hat{C}(x_{i}),\hat{C}(y_{i}),\theta^{(t)}) + \epsilon_i\|\]
If $\epsilon_i$ is unbiased (averages 0 over all data points) and
varies less than gradient:
\[
\mathbb{E}(\|\epsilon\|^2) \le \mathbb{E}(\|\nabla\|^2)
\]
then, $\sigma_{imp} \le \sigma_{uni}$.
\end{theorem}
\begin{proof}
\reminder{Probably cite TR for proof}
\end{proof}

From this analysis, we see that the allowed error in $\hat{C}$ can be quite large. 
As long as on average we estimate the right result, we can make mistakes on the order of 
the gradient's norm.
We will use this insight to design a cheap coarse grained estimator than relies on error
classification (i.e. is the point erroneous) rather than trying to learn a complicated function
that predicts the clean value.

\subsection{Error Predicate Case}
If errors are sparse, then the previous algorithm still wastes a lot of computation.
For example, for most of the data points:
\[\nabla\phi(x_{i},y_{i}, \theta) = \nabla\phi(C(x_{i}),C(y_{i}),\theta)\]
Essentialy, we are computing the gradient at many points and then throwing that computation away when we sample.
We now explore how we can use this previous computation to further reduce the variance $\sigma^2$.

In Section \ref{api}, we described the basic API and user interface of our system.
An analyst specifies multiple cleaning operations and then a model.
Suppose, we decomposed the function $C$ into its consituent cleaning operations ${C}_i^{k}$.
To describe the parse errors, for each $C_i$ there is some predicate $\rho_i$ that selects the affected records.
The next case that we analyze is when we know the perfectly $\rho_i$ in advance.
Unlike the oracular case, there are some real-world situations where this true.
For example, if data is missing e.g., field misalignment it is often clear which rows are affected and which are not.
Similarly, in the entire field of constraint-based data cleaning \cite{nadeef}, we know in advance which records have constraint violations.

These predicates also simplify the construction of the estimator $\hat{C}$. 
For a tuple $(x_j,y_j)$, there are basically two cases:
\[
(x_j,y_j)= \begin{cases} (x_j,y_j) &\rho_i \text{ is false} \\ 
(C_i(x_j),C_i(y_j)) & \rho_i \text{ is true} 
\end{cases} 
\]
We can calculate the average change over already cleaned data that satisfy the predicate:
\[
\Delta_i = \sum_{j \in \rho(cleaned)} (C_i(x_j),C_i(y_j))
\]
the resulting estimator is:
\[
\hat{C}_i= \begin{cases} (x_j,y_j) &\rho_i \text{ is false} \\ 
(x_j,y_j) + \Delta_i & \rho_i \text{ is true} 
\end{cases} 
\]

The oracular algorithm gets modified as follows:
\begin{enumerate}[noitemsep]
\item Initialize all $\Delta = 0$
\item For rounds i=1...T
\begin{enumerate}
	\item For each cleaning op j=1...K
	\begin{enumerate}
		\item For all points that do not satisfy $\rho_j$, calculate the average gradient and call this result $g_1$.
		\item For all points that do satisfy $\rho_j$, sample $\frac{B}{T}$ data points using $\hat{C}_j$ described above.
		\item Apply data cleaning to the sample of data, or if already cleaned skip.
		\item For all newly cleaned points update $\Delta_j$
		\item Calculate weighted gradient of the sample points call this $g_2$.
		\item For $N_1$ points that do not satisfy the predicate and $N_2$ points that do,  apply gradient descent using the gradient $\frac{N_1g_1 + N_2g_2}{N_1 + N_2}$.
	\end{enumerate}
\end{enumerate}
\item Return $\theta^{(T)}$
\end{enumerate} 

To actually implement this algorithm efficiently there are physical design considerations that we will discuss in the next section.

\subsubsection{Error Predicate Case: Analysis}
Using the error predicates can significantly increase the accuracy when errors are sparse.
In fact, this saving is quadratic in the sparsity $O(sparsity^2)$.
\begin{theorem}
The error-predicate algorithm converges to the correct answer.
Furthermore, suppose there are $N_1$ erroneous points out of $N$ total points.
\[\sigma^2_{ep} = \frac{N^2_1\sigma^2_{imp}}{N^2}\]
\end{theorem}
\begin{proof}
Gradient is unbiased at each step.
\reminder{Probably cite TR for proof}
\end{proof}

\begin{theorem}
$\Delta_i$ error satisfies variance conditions under NormalizedSC like conditions.
\reminder{Do the analysis}
\reminder{Also number of rounds before this happens}
\end{theorem}
\begin{proof}
\reminder{Probably cite TR for proof}
\end{proof}

\subsection{General Case}
Finally, in the general case, we do not know the error predicate in advance.
Consider a crowd-based cleaning technique whose effects are hard to predict a priori. 
In this case, we have to continuously update a classifier to partition the data into 
clean and dirty.
The challenge is that this potentially biases our result if our classifier is not perfect.
Some dirty points may be classified as clean.
The quadratic savings seen in the predicate case can often outweigh small biases due to classification error.

Like before, suppose we decomposed the function $C$ into its consituent cleaning operations ${C}_i^{k}$.
For each cleaning operation, we have a classifier, e.g., Support Vector Machine, that learns which points are dirty and which points are cleaned based on that operation.
To account for potential bias in the classifier, we allow the analyst to specify the design point in the bias variance tradeoff.
Many types of classifiers allow users to tradeoff precision and recall.
For an SVM, we may only classify a point as clean if it is sufficiently far from the margin.
Or for Logistic Regression, we may do so if its class likelihood is over 80\%.
In our experiments, we use SVMs and the margin distance provides a flexible way to tradeoff potential bias and gradient variance.

Adding the classifier also adds a complex depedence between batch size and performance.
Initially, the classifier is likely very inaccurate due to the lack of training examples.
As we clean more data, we can learn what is clean and what is dirty.
Accordingly, we can delay the classifier's introduction by $l$ iterations until such time it is accurate. 

\begin{enumerate}[noitemsep]
\item Initialize all $\Delta = 0$
\item For rounds i=1...T
\begin{enumerate}
	\item For each cleaning op j=1...K
	\begin{enumerate}
		\item If $i \ge l$, apply classifier to partition dirty and cleaned data.
		\item Follow error predicate algorithm steps ii-v
		\item For $N_1$ ``clean" points and $N_2$ ``dirty" points, apply gradient descent using the gradient $\frac{N_1g_1 + N_2g_2}{N_1 + N_2}$.
		\item Update classifier with newly cleaned data.
	\end{enumerate}
\end{enumerate}
\item Return $\theta^{(T)}$
\end{enumerate} 

The general case algorithm is harder to analyze and thus we rely on empirical performance on real data.




