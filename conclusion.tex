\section{Conclusion}
In this paper, we propose \sysfull (\sys), an anytime framework for training Machine Learning models with a data cleaning budget.
Naive solutions to this problem can cause sampling errors to dominate any benefit of data cleaning, so instead we propose a gradient-based update to incrementally correct a dirty model.
To make this update as impactful as possible, we exploit many properties we know about data cleaning such as the relative ease of error enumeration.
Our solution is a linear approximation of the optimal sampling distribution which empirically shows significant improvements over alternative approaches.
This formulation fits into a Stochastic Gradient Descent theoretical framework, which allows us to ensure convergence and statistical consistency under relatively mild conditions.
The elegance of the SGD formulation is that we can use approximations of approximations and still have a methodology that gives bounded results.

\sys is only a first step in a larger integration of data analytics and data aquisition/cleaning. 
There are several exciting, new avenues for future work.
First, in this work, we largely study how knowing the Machine Learning model can optimize data cleaning.
We also believe that the reverse is true, knowing the data cleaning operations and the featurization can optimize model training.
For example, applying Entity Resolution to one-hot encoded features results in a linear transformation of the feature space.
For some types of Machine Learning models, we may be able to avoid re-training.
The optimizations described in \sys are not only restricted to SGD algorithms. 
We believe we can extend a variant of SDCA (Stochastic Dual Coordinate Ascent) \cite{jaggi2014communication} to extend this technique to kernelized methods.
