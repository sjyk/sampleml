We would like to thank the reviewers and meta-reviewer for
their thorough feedback on this paper.
We have significantly revised and clarified the presentation and exposition
of the paper.
First, we would like to clarify a key point about the nature of our system.
While a number of recent works apply Machine Learning to more efficiently repair dirty data, in this work, we explore the problem of data cleaning (learning-based or otherwise) prior to Machine Learning model training.
We have clarified this point in our introduction:

\vspace{0.25em}

\emph{This paper explores progressive data cleaning when there is a single predictive model at the end of the analysis pipeline.
We focus on two problems for a popular class of models called convex loss models (e.g., linear regression and SVMs): (1) the methodological problem of reliable analysis with progressive data cleaning and predictive modeling, and (2) the algorithmic problem of using information from the model to prioritize data cleaning.
For (1), the key challenge is that predictive model training often implicitly solves an optimization problem which does not commute with progressive data cleaning.
We show that instead of re-optimizing, the model can be incrementally maintained with gradient descent given newly cleaned data, an technique that is guaranteed to converge under suitable conditions.
To address challenge (2), similar to Active Learning, we propose a sampling algorithm that selects the most valuable records to clean with higher probability.
However, an important distinction with prior work is that this prioritization utilizes information from the downstream model.}

\vspace{0.25em}

\noindent\textbf{Meta Review Details} 

\vspace{0.25em}

\noindent \textbf{There should be a formal vocabulary introduced early on. The exact idea of ``dirty" here can be hard to follow: what is the exact error type(s) that the system is intended to clean?}

\vspace{0.25em}

We thank the reviewers for this important feedback and clarified that our system applies to data cleaning operations that can be modeled as record-by-record mappings between two relations with the same schema.
This model is sufficiently expressive for our experiments and a number of real-world dirty data scenarios such as attribute value canonicalization and missing value filling.
We hope to explore additional data cleaning models that include record deduplication or schema mapping in future work.
We added the following clarification to Section 2.3:

\emph{This paper explores a model for data error where attributes of records in a relation $R$ are incorrect or missing.
Formally, there are two relations $R_{dirty}$ and $R_{clean}$ with the same schema and a one-to-one mapping between rows.
Therefore, for every $r \in R_{dirty}$ there exists an $r' \in R_{clean}$, and a cleaning function $C(\cdot)$, is a function that maps each $r$ to its corresponding $r'$.
We assume that there is a featurization $F(\cdot)$ which is defined over both $R_{dirty}$ and $R_{clean}$ and maps rows to vectors in $\mathbb{R}^d$.
So each row corresponds to one training example in the predictive model.\\
Consider the data corruption in our motivating example where company names were inconsistently entered: ``Pfizer Inc.", ``Pfizer Incorporated", ``Pfizer".
Fixing this error can be represented as a record-by-record mapping.
The data cleaning function would pick one canonical representation for the company (e.g. ``Pfizer Inc.") and map all records with other values that refer to the same real-world entity to the canonical representation.
However, there are types of data cleaning that do not satisfy this model such as schema mapping, record deduplication, and data extractions that create additional columns.}

\vspace{0.25em}

\textbf{Sections 5-7 are the technical core of the paper, and appear formal at the expense of aiding understanding. They appear to implement something that resembles active learning or bootstrapping, except inside the gradient descent loop. The motivation of some of this is not clear; is it necessary to integrate with the gradient descent? This is not how most active learning methods are implemented. Is it possible to implement these approaches in a way that is orthogonal to the SGD algorithm? The current writeup entangles some of these design choices.} 

In the introduction of the paper, we have clarified that ActiveClean we explores the problem of data cleaning prior to Machine Learning model training where the model is analyst-specified data analytics. In other approaches that apply Active Learning to data cleaning \cite{gokhale2014corleone, DBLP:journals/pvldb/YakoutENOI11, yakout2013don}, the machine learing is \emph{internal} to the data cleaning to make it more efficient. For example, in the process of deduplication, one can train a model to learn to score features of pairs of records. 
This model can be trained with Active Learning to require as few labeled pairs as possible.
 In contrast, we consider leveraging the fact that the endpoint of many data analysis pipelines is a predictive model, and information from this model can be used to budget data cleaning.
An example of this is a recommender system whose user-item matrix has missing values due to a system failure, and these values need to be imputed using logs.
Here, the learning is \emph{external} to the data cleaning. 

In particular, we apologize for conflating two separate problems:

\vspace{0.25em}

\emph{(1) the methodological problem of reliable analysis with progressive data cleaning and predictive modeling, and (2) the algorithmic problem of using information from the model to prioritize data cleaning} 

\vspace{0.25em}

We re-organized the paper to first describe the solution to problem (1). We argue that without an incremental optimization approach, there is a methodological problem of model training after progressive data cleaning. Figure \ref{update-arch1} describes straight-forward applications of progressive data cleaning and model training and why these approaches have issues.
These issues can occur whether or not the data cleaning is interactive or passive, as the problem is not with the data cleaning but with the model updating procedure.
The basic problem is that given the new data the model has to be incrementally updated rather than training on a mix of dirty and clean data.
The integration with SGD is one approach to addressing problem (1), and actually any incremental optimization technique can apply such as proximal methods and primal-dual methods.

\reminder{Add verbatim fixes here}

The subsequent sections describe an algorithmic solution to problem (2). These approaches are orthogonal to the model updating problem and can apply with any incremental update framework. However, we motivate our specific choice of prioritization by describing optimality with respect to the SGD update.
In our revision, we have abstracted these two separate problems and describe them independently.

\reminder{Add verbatim fixes here}

\vspace{0.25em}

\textbf{In general, the distinction between an ``architecture" and an ``algorithm that fits into the architecture" is quite unclear. The problem with SGD/Active Learning above is one example.}

As mentioned in the previous response, we have addressed this problem by separating the methodological problem and the algorithmic problem. ActiveClean is an architecture which given a downstream predictive model and a record-by-record data cleaning function, applies progressive data cleaning in a way that results in reliable intermediate results.
We describe the optimizations to the approach such as importance sampling and detection as orthogonal algorithms that fit into the framework.

\reminder{Add verbatim fixes here}

\vspace{0.25em}

\textbf{The paper, and especially the technical sections, would benefit enormously from a detailed running example showing how the algorithm works}

We appreciate this feedback and we have added simplified linear regression running example with illustrations. 

\reminder{TODO}

\vspace{0.25em}

\textbf{Some connections to related work that combines machine learning and data cleaning should be made. See the other reviewers' comments.}

We thank the reviewers for suggested highly relevant related work, and now highlight these important results in the introduction:

\emph{When data cleaning is expensive, it is desirable to apply it \textbf{progressively}, where analysts can inspect early results with only $k \ll N$ records cleaned.
Progressive data cleaning is a well studied problem especially in the contex of entity resolution \cite{whang2014incremental, papenbrock2015progressive, gruenheid2014incremental}.
Increasingly, Active Learning \cite{settles2010active} or other statistical methods are applied to select records or contraint violations to clean in a way that maximizes the information gained \cite{DBLP:journals/pvldb/YakoutENOI11, gokhale2014corleone, yakout2013don}.\\
However, these frameworks do not consider the requirements of the downstream applications which can give valuable information on when and how to apply data cleaning.
While using downstream query semantics has been explored in the context of conjuctive queries \cite{DBLP:conf/sigmod/BergmanMNT15} and SQL aggregates \cite{wang1999sample}, it is important to recognize the growing popularity of predictive models in data analytics \cite{bdas, alexandrov2014stratosphere, crotty2014tupleware, hellerstein2012madlib}.
Predictive models rely on learning relationships between features and labels, and systematic data corruption \cite{taylor1982introduction} can mask or even introduce spurious new relationships.
Furthermore, the high dimensionality of these models can amplify small problems \cite{xiaofeature} resulting in error-prone predictions even when trained on mostly clean data.}

\vspace{0.25em}

We also added a detailed clarification in Section 2.2:

\vspace{0.25em}

\emph{There are also a number of recent approaches that apply Active Learning to reduce number of records that need to be cleaned by a human.
For example, Gokhale et al. \cite{gokhale2014corleone} uses Active Learning to select the most valuable pairwise comparisions to identify duplicates.
Yakout et al. \cite{yakout2013don,DBLP:journals/pvldb/YakoutENOI11} uses information theoretic and statistical criteria to prioritize data repair.
The key distinction is that in these problems Machine Learning is a part of the data cleaning technique.
Superivised machine learning methods are used to repair the data from training examples.
In our problem, the data cleaning, whether learning-based or otherwise, is external and at the end of the analysis pipeline.
In other words, we explore the problem of model training \emph{after} data cleaning as opposed to using a model \emph{for} data cleaning.\\
Our first goal is to find an update methodology that avoids Simpson's Paradox and the strong dependence on sample size.
Instead of mixing dirty and clean data, \sys uses a model trained on the dirty data as an initialization, and then iteratively updates this model using samples of clean data.
The intuition is that this algorithm smoothly transitions the model from one population (the dirty data) to another (the clean data), leading to provable guarantees about intermediate results.
Our next goal is to priotize which data to clean based on information from the model.
We design \sys to make greater progress at these small sample sizes using the dirty model as an initialization.
Doing so is not trivial since data may look unimportant to a dirty model but when cleaned are very important.
Also, data cleaning and model training can happen at very different time scales, we have to carefully budget our effort to ensure that any optimizations actually address rate-determining steps in the workflow.}