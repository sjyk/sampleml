\section{Update}\label{model-update}
Before we discuss sampling, we will first discuss how to update a model.
We will assume that our detection step in the previous section has selected 
a set of candidate records $R_{dirty}$ and our sampling step has sampled from this
set of candidate records.
We will show that this model update procedure can be interpreted as a Stochastic 
Gradient Descent (SGD) algorithm, which gives us a theoretical framework to analyze
convergence and bound the error at each step.

\subsection{Update Problem}
The high level challenge is that we want a model update technique that takes advantage of the entire dirty data and a small sample of cleaned data.
We want this technique to have some provable guarantees like the conditions under which we converge to the right answer as the entire data is cleaned, bounds on error of intermediate results, and a guarantee that cleaning will improve the error.  

First, let us describe the abstract problem.
Suppose we have a cleaned batch of data $S_{clean}$ with features and labels $(X_{clean},Y_{clean})$, and a dirty model $\theta^{(d)}$. 
In the model update problem, we update the dirty model with some function $f(\cdot)$ i.e.,:
\[
\theta^{new} \leftarrow f(X_{clean},Y_{clean},\theta^{(d)})
\]
Our goal is that these updates should minimize the error of the updated model and the true model $\theta^{(c)}$ (if we cleaned and trained over the entire data):
\[
error(\theta^{new}) = \| \theta^{new} - \theta^{(c)} \|
\]

There are a couple of challenges that we address in this section.
As described in our architecture, we do not want to clean data that we have already cleaned or is expected to be clean.
This leads to our first challenge since the sample $S_{dirty}$ may not be representative of the full data, only $R_{dirty}$. 
We have to design our update in such a way that it does not bias our result towards the newly cleaned data. 
The second challenge is that we need to get the sample $S_{dirty}$ from $R_{dirty}$.
Some records, if cleaned, are more informative than others.
We show how to incorporate non-uniform sampling into our update and formulate the theoretical optimal sampling problem.
In the next section, we will discuss how to practically address the optimal sampling problem.

\subsection{The Geometry Of The Problem}
Let us try to understand the geometry of this problem in one dimension (i.e., the parameter $\theta$ is a scalar vale).
We restrict the class of Machine Learning models to convex regularized loss problems.
That is, as we vary the model $\theta$ the loss is bowl-shaped (visualized in Figure \ref{update-arch2}a).
There is a unique optimal value for $\theta$ at the bottom of the bowl.
If we have dirty data, we are training a model with respect to an incorrect data distribution.
In other words, we are optimizing the wrong function (Figure \ref{update-arch2}b).
The resulting dirty model can be thought of as a suboptimal point on clean curve.

\begin{figure}[ht!]
\centering
 \includegraphics[width=\columnwidth]{figs/update-arch2.png}
 \caption{In \sys, we explore an important class of Machine Learning problems where the loss function (i.e., training error) varies convexly. An incorrect model can be thought of as a sub-optimal point. \label{update-arch2}}
\end{figure}

We have a model $\theta^{(d)}$ (visualized in brown) that is suboptimal with respect to the clean data.
We want to get to the optimal clean model $\theta^{(c)}$ which is visualized as a yellow star.
If we had all of the clean data we could compute $\theta^{(c)}$.
However, at each iteration, we only have a sample of clean data.
To resolve this issue, we can exploit the model's geometry.
For this class of models, given a suboptimal point, we can find the direction to the 
the global optimum.
Mathematically, this direction corresponds to gradient of the loss with respect to $\theta$, and we need to move some distance $\gamma$ along this direction:
\[
\theta^{new} \leftarrow \theta^{(d)} - \gamma \cdot \nabla\phi(\theta^{(d)})
\]
In our visualization, this corresponds to whether we should move left or right.
The intuition, which we will formalize in Section \ref{sgd}, is that if we are on average in the right direction the algorithm is guaranteed to converge with analytics bounds on the convergence rate.
At the optimal point, the expected gradient will be zero.
So intuitively, this approach iteratively moves the model downhill, or corrects, the dirty model until the budget is reached.

\subsection{Average Gradient From a Sample}
Our class of Machine Learning models are based on loss minimization, that is they are a sum of losses, and we know that sums commute with derivatives.
So to calculate the average gradient from a sample, we simply calculate the gradient at every data point and average over them.
Let $S$ be a sample of data, where each $i \in S$ is drawn with probability $p_i$.
We can approximate the global gradient:
\[
\nabla\phi(\theta) \approx g_{S}(\theta) = \frac{1}{n\mid S \mid} \sum_{i \in S}\frac{1}{p_i}\nabla\phi(x_i^{clean},y_i^{clean},\theta)
\]
Then for every batch of data cleaned, we apply the update to the current best model estimate:
\[
\theta^{(t)} \leftarrow \theta^{(t-1)} - \gamma \cdot g_{S}(\theta^{(t-1)})
\]

The tricky part is that only a subset of the records are are cleaned.
The consequence of this is that the estimate $g_{S}(\theta)$ is now biased.
We also have to compensate for this bias by averaging this estimate with the gradient with respect to the clean data:
\[
g_C(\theta) = \frac{1}{\mid R - R_{dirty}\mid}\sum_{i \in R - R_{dirty}}\nabla\phi(x_i^{clean},y_i^{clean},\theta)
\]
Then, for weights $\alpha,\beta$ which we will subsequently discuss how to select:
\[
g(\theta) = \alpha \cdot g_C(\theta) + \beta \cdot g_S(\theta)
\]
So then our final gradient update becomes:
\[
\theta^{(t)} \leftarrow \theta^{(t-1)} - \gamma \cdot g(\theta) \blacksquare
\]

\subsection{Model Update Algorithm}
Following from this geometric analysis, we propose the iterative model correction algorithm.
We initialize the algorithm with $\theta^{(d)}$ which is the dirty model.
At each iteration $i=\{1,...,t\}$, we clean a batch of data $b$ selected from the set of candidate dirty records $R_{dirty}$.
Then, starting with $\theta^{(d)}$, we apply an averaged gradient update to get $\theta^{(i)}$.
We iterate until our budget of cleaning $k = t \cdot b$ record is reached.

We present the model update algorithm here:
\begin{enumerate}[noitemsep]
	\item Calculate the gradient over the sample of clean data and call the result $g_S(\theta^{(i-1)})$
	\item Calculate the average gradient over all the data in $R_{clean}$, and call the result $g_C(\theta^{(i)})$
	\item Apply the following update rule:
	\[
	\theta^{(i+1)} \leftarrow \theta^{(i)} - \lambda \cdot(\alpha\cdot g_S(\theta^{(i)}) + \beta \cdot  g_C(\theta^{(i)}))
	\]
\end{enumerate} 

\subsubsection{Selecting the Parameters}
The proposed update policy makes intuitive sense.
However, we have still have to set the parameters $\lambda$, $\alpha$, $\beta$.
We will show that theory from the stochastic optimization literature can allow us to understand this iterative algorithm.
First, we will review all of the parameters that need to be set.

\vspace{0.5em}

\noindent\textbf{Step Size $\gamma$ : } The first problem is that we have not explained how to pick the step size $\gamma$. In other words, how far should we travel in the gradient direction.

\vspace{0.5em}

\noindent\textbf{Step Size $\alpha,\beta$ : } The next problem is deriving the proportions with which we should combine $g_S(\theta)$ and $g_C(\theta)$. It turns out that $\alpha = \frac{R_{clean}}{R}$ and $\beta = \frac{R_{dirty}}{R}$, and we will show a derivation below.

\vspace{0.5em}

\noindent\textbf{Choosing $S$: } As we mentioned, the optimal clean model depends on \emph{all} the clean data, not just a sample. 
So $g_S$ is really an approximation of the the true gradient $g^*$ with some error $g^* \pm \epsilon$. 
The quality of the update depends on how well we can approximate $g^*$ using $g_S$.
The problem is how should we construct the sample of data to clean $S$ to get the most accurate update.
In particular, how should we choose the sampling probabilities $p_i$ for each $i \in S$ such that the error is minimized.

\subsection{Analysis with Stochastic Gradient Descent}\label{sgd}
This update policy can be formalized as a class of very well studied algorithms called Stochastic Gradient Descent.
This gives us a theoretical framework to understand and analyze our update rule, bound the error, and choosing points to clean.
Mini-batch stochastic gradient descent (SGD) is an algorithm for finding the optimal value
of $\theta$, given the convex loss, and data.

\vspace{0.5em}

\noindent\textbf{ Setting $\gamma$: } There is extensive literature in machine learning for choosing $\gamma$ appropriately. $\gamma$ can be set either to be a constant or decayed over time. Many machine learning frameworks (e.g., MLLib, Sci-kit Learn, Vowpal Wabbit) automatically set learning rates or provide different learning scheduling frameworks. 
In our experiments, we use a technique called inverse scaling where there is a parameter $\gamma_0=0.1$, and at each iteration we reduce it to $\gamma_t = \frac{\gamma_0}{\mid S \mid t}$. 

\vspace{0.5em}

\noindent\textbf{ Convergence: } The next property of concern is convergence. Convergence properties of batch SGD formulations has been well studied \cite{dekel2012optimal}. 
The conditions on this convergence are essentially that at each step the estimate of the gradient $g_S$ has to be unbiased, that is on average correct. 

\begin{proposition}
For an appropriately chosen learning rate $\gamma_t$, batch stochastic gradient descent will converge if $\mathbb{E}(g_S)=g^*$.
\label{unbiased}
\end{proposition}

A direct result of convergence is a guarantee on statistical consistency.
Another interesting consequence of this result is that we can sample $S$ in any way that we want
as long as our estimate in unbiased.

\vspace{0.5em}

\noindent\textbf{ Convergence Rate: } The convergence rates of SGD are also well analyzed \cite{dekel2012optimal,bertsekas2011incremental,zhao2014stochastic}. 
Suppose, a user cleans a batch size of $b$ examples at each iteration.
This allows us to bound the error of intermediate models and understand the expected number of steps before a model within a certain error. 

\begin{proposition}
For a general convex loss, a batch size $b$, and $t$ iterations, the convergence rate is bounded by $O(\frac{\sigma^2}{\sqrt{bt}})$. 
$\sigma^2$ is the variance in the estimate of the gradient at each iteration:
\[
\mathbb{E}(\|g_S - g^*\|^2)
\]
\end{proposition}

There is an interesting tradeoff between batch size an convergence rate.
Increasing the batch size reduces the variance $\sigma^2$, however it does
mean at each iteration you are doing more work.
In a data cleaning context, this is a problem since the bottlneck is the work at 
each iteration not the number of iterations.

\vspace{0.5em}

\noindent\textbf{ Laziness Argument: } The convergence and the error bounds show that the technique is theoretically justified, but the remaining question is whether the constant factors make this technique practical.
It is well known that even for arbitrary initializations SGD makes significant progress in less than one epoch (a pass through the entire dataset) \cite{bottou2012stochastic}.
In our case, we often have a decent initialization which is the dirty model, which means that we can get close processing far less than the full data.
Our algorithm is essentially an arguement for lazy materialization, where we only clean the data when needed.

