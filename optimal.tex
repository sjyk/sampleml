\section{Sampling and Model Update}
In this section, we describe how we combine the different components
of our system.
If we formulate the model update step in \sys as a Stochastic Gradient Descent step, then 
it provides us with a framework to derive an update policy (Problem \ref{imp-update}), and in turn how that informs our sampling distribution (Problem \ref{imp-samp}).

\subsection{Stochastic Gradient Descent}
Mini-batch stochastic gradient descent (SGD) is an algorithm for finding the optimal value
of $\theta$, given the convex loss, and data.
At each iteration, SGD draws a subset of data at random and update with the average gradient of that subset of data.
 \[
 \theta^{(t+1)}\leftarrow\theta^{(t)}-\gamma\frac{1}{\mid S^{(t)}\mid}\sum_{i\in S^{(t)}}\nabla\phi
 \]
$\gamma$ is a hyperparameter which is called the learning rate.
There is extensive literature in machine learning for choosing $\gamma$ appropriately, and in this work, we will assume that it is set by the system.

We treat the model update problem problem as a SGD problem.
We start with an initialization which is the dirty model, the model trained completely on the dirty data.
Then, at each iteration we sample a mini-batch of data on which we apply our expensive data cleaning operations.
We update the model using an SGD step (see \cite{bertsekas2011incremental} for a survey of Incremental optimization methods).

\noindent\textbf{SGD Model Update (Problem \ref{imp-update}): }
\begin{enumerate}
\item For a sample of data $S$ from $R_{dirty}$, calculate the expected gradient with respect to the sampling distribution (Section \ref{dist-samp}). Call this value $g_d^{(t)}$.
\item For the clean data $C = R - R_{dirty}$, calculate the population mean gradient $g_c^{(t)}$.
\item Apply gradient descent using the gradient $g = \frac{\mid C \mid g_d + \mid S \mid g_c}{\mid C \mid  + \mid S \mid}$
\end{enumerate}

\noindent We stop when the budget is reached returning the best model at that time.
We argue that if the initial dirty model is relatively close the true model, a small number of gradient steps with clean data can compensate for the errors.

Here, we can draw an analogy to Materialized View maintenance, since after all, a model parameterized by $\theta$ is just a table of floating point numbers.
Krishnan et al. proposed a technique called sample view cleaning, in which they take a clean sample of data and propogate the updates to a Materialized View.
Similarly, in this work, we take the information from a sample of cleaned data and propagate an update with the gradient.
The insight that many mathematical problems are analogous to view maintenance has recently been noted others as well \cite{nikolic2014linview}. 

However, it is important to note, that this analogy only goes so far.
Most incremental view maintenance architectures can propagate their updates in a single pass of the data.
In our setting, this is not guaranteed. 
If the dirty model greatly differs from the clean model a single epoch may not be enough to reconcile the differences.
In this case, to avoid cleaning the entire data, it may be better to take a clean sample of data and train the model completely rather than doing it in an iterative fashion.
We explore this tradeoff in our experiments \reminder{[?]}.

\subsection{Sampling Distribution}\label{dist-samp}
In the Machine Learning and Optimization literature, SGD algorithms are optimized to avoid scanning the entire data.
However, in the data cleaning setting, there is a new bottleneck.
Data cleaning is often the most expensive step in the workflow, so optimizing for scan cost may lead to neglible overall improvements.
Instead, in \sys, we sacrifice a small overhead in pre-computation for each data point to determine its value to the model and select a sampling distribution accordingly.
Intuitively, while each iteration has an increased cost, it also makes more progress towards the optimum.

To construct this distribution, we have to analyze a single SGD iteration.
In essence, the key property that allows SGD to converge is that the gradient step $g^{(t)}$ defined as:
\[
g^{(t)} = \frac{1}{\mid S^{(t)}\mid}\sum_{i\in S^{(t)}}\nabla\phi
\]
is an unbiased estimate of the ``true" gradient (i.e., over all training examples):
\[
\mathbb{E}(g^{(t)}) = \frac{1}{N}\sum\nabla\phi
\]
With uniform sampling this basic SGD algorithm has performance lower bound \footnote{there is a technical condition that this bound holds for strongly convex functions}:
\[
\mathbb{E}(\|\theta^{(t)}-\theta^{*}\|^{2})\le O(\frac{\sigma^{2}}{bt})
\]
%and for the non-strongly convex case \reminder{explain better}
%\[
%\mathbb{E}(\|\theta^{(t)}-\theta^{*}\|^{2})\le O(\frac{\sigma^{2}}{\sqrt{bt}})
%\]
where $\mathbb{E}(\|\nabla\phi\|_{2}^{2})=\sigma^{2}$.
If we choose our sampling technique carefuly, we can reduce the $\sigma^{2}$ term in the error bound.
We preserve the same expected gradient at each step $\mathbb{E}(g^{(t)})$ but possibly have a lower variance.

The technique that we will apply is called importance sampling.
Importance sampling is a technique that allows us to draw a batch from a non-uniform distribution but reweight the result to give us an unbiased estimate of the gradient.
It is established \cite{zhao2014stochastic} that the optimal sampling distribution (lowest variance) is:
\[p_{i}\propto\|\nabla\phi(x,y,\theta^{(t)})\|\]
There is immediate a chicken-or-egg problem with this optimal distribution, namely it is optimal if we know the gradient; which is exactly what we are trying to estimate.
This problem is further complicated by the fact that we do not know the actual value
of the record until we clean it.
In our setting, we are actually calculating the gradient of the cleaned data:
\[\phi(x_{clean},y_{clean},\theta^{(t)})\]

To address these problems, we will show how we can use the structure of the data cleaning specifications in the previous section to compute low cost estimates of the cleaned value.
In other words, let $e(\cdot)$ be an estimator function, then, we calculate the sampling distribution: 
\[p_{i}\propto\|\nabla\phi(e(x,y),\theta^{(t)}) \|\]
The gradient update becomes:
\[g_d^{(t)} = \frac{1}{\mid S^{(t)} \mid}\sum_{i\in S^{(t)}}\frac{1}{p_{i}}\nabla\phi(x_{i},y_{i},\theta^{(t)})\]
In our analysis, we will describe the conditions under which this distribution has lower variance than uniform sampling.

\subsection{Summary}
In summary, while we still have to discuss how we find the estimator function $e(\cdot)$, we can describe the basic processing workflow of \sys.
\begin{enumerate}[noitemsep]
\item Initialize with $\theta^{(0)}$ as the dirty model, $T$ iterations, with a batch size $B$
\item For rounds i=1...T
\begin{enumerate}
	\item Sample $B$ candidate dirty data points with probabilites as described.
	\item Apply data cleaning to the sample of data.
	\item Apply weighted gradient descent to update the model.
\end{enumerate}
\item Return $\theta^{(T)}$
\end{enumerate} 