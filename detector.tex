\subsection{The Detector}\label{det}
% Partitioning Data With Detection}\label{det}
% \sys improves the progress of each update iteration by intelligently partitioning the data into expected dirty and clean partitions. 
% This ensures that sampling draws data that are likely to be dirty.

% \subsubsection{Goals}
The detector returns two important aspects of a record: 
(1) whether the record is dirty, and (2) if it is dirty, what is wrong with the record.
The sampler can use (1) to select a subset of dirty records to sample at each batch and 
the estimator can use (2) estimate the value of data cleaning based on other records with the same corruption.
\sys supports two types of detectors: \emph{a priori} and \emph{adaptive}.
In former assumes that we know the set of dirty records and how they are dirty \emph{a priori} to \sys,
while the latter \emph{adaptively} learns characteristics of the dirty data as part of running \sys.

\subsubsection{\protect\textit{\large A Priori} Detector}
For many types of dirtiness such as missing attribute values and constraint violations, 
it is possible to efficiently enumerate a set of corrupted records and determine how the records are corrupted.

\begin{definition}[A Priori Detection]
Let $r$ be a record in $R$. An a priori detector is a detector that returns a Boolean of whether the record is dirty and a set of columns $e_r$ that are dirty.
\[
D(r) = (\{0,1\}, e_r)
\]
From the set of columns that are dirty, find the corresponding features that are dirty $f_r$ and labels that are dirty $l_r$.
\end{definition}

\noindent Here are example use cases of this definition using data cleaning methodologies proposed in the literature.

\vspace{0.5em}

\noindent\textbf{Constraint-based Repair: }
One model for detecting errors involves declaring constraints on the database.

\vspace{0.5em}

\emph{Detection. } Let $\Sigma$ be a set of constraints on the relation $\mathcal{R}$. 
In the detection step, the detector selects a subset of records $\mathcal{R}_{dirty} \subseteq \mathcal{R}$ that violate at least one constraint.
The set $e_r$ is the set of columns for each record which have a constraint violation. 

\begin{example}\label{detex1}
An example of a constraint on the running example dataset is that the \texttt{status} of
a contribution can be only ``allowed" or ``disallowed".
Any other value for \texttt{status} is an error.
\end{example}

\subsubsection{Adaptive Detector}
\emph{A priori} detection is not possible in all cases.
The detector also supports adaptive detection where detection is learned from previously cleaned data.
Note that this ``learning" is distinct from the ``learning" at the end of the pipeline.
The challenge in formulating this problem is that detector needs to describe how the data is dirty (e.g. $e_r$ in the \emph{a priori} case).
The detector achieves this by categorizing the corruption into $u$ classes.
These classes are corruption categories that do not necessarily align with features, but every record is classified with at most one category.
For example, suppose there are records with outliers and missing values, there are three classes of corruption: outliers, missing values, and both.

\reminder{Check if it really needs to be combinatorial}
While the number of corruption classes is combinatorial in the individual types, the only classes that need to enumerated are ones that are manifest in the data.
When using adaptive detection, the repair step has to clean the data and report to which of the $u$ classes the corrupted record belongs.
When an example $(x,y)$ is cleaned, the repair step labels it with one of the ${\text{clean}, 1,2,...,u+1}$ classes (including one for ``not dirty").
It is possible that $u$ increases each iteration as more types of dirtiness are discovered. 
Then, the detection problem reduces to a multiclass classification problem.
This problem can be addressed by any multiclass classifier, and we use an all-versus-one SVM in our experiments.

\reminder{Are there different types of adaptive detectors we test?  If so, we should list them out here and talk about tradeoffs/when they are useful!}
\reminder{Also, the learning problem here sounds hopeless (sparse), need to argue why this even makes sense}

\begin{definition}[Adaptive Detection]
Select the set of records for which $\kappa$ gives a positive error classification (i.e., one of the $u$ error classes).
After each sample of data is cleaned, the classifier $\kappa$ is retrained.
So the result is:
\[D(r) = (\{1,0\},\{1,...,u+1\})\]
\end{definition}

\vspace{0.75em}

\noindent\textbf{Adaptive Detection With OpenRefine: }
\begin{example}\label{detex2}
OpenRefine is a spreadsheet-based tool that allows users to explore and transform data.
However, it is limited to cleaning data that can fit in memory on a single computer.
Since the cleaning operations are coupled with data exploration, \sys does not know what is dirty in advance (the analyst may discover new errors as she cleans).

Suppose the analyst wants to use OpenRefine to clean the running example dataset with \sys.
She takes a sample of data from the entire dataset and uses the tool to discover errors.
For example, she finds that some drugs are incorrectly classified as both drugs and devices.
She then removes the device attribute for all records that have the drug name in question.
As she fixes the records, she tags each one with a category tag of which corruption it belongs to.
\end{example}






