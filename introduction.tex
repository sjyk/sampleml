\section{Introduction}
Data are susceptible to various forms of corruption, or \emph{dirtiness}, such as missing, incorrect, or inconsistent values.
Numerous surveys of industry have shown that dirty data are prevelant \cite{Gartner}, and there is now a growing industry around processing dirty data at scale \cite{fortunearticle}.
Analysts are increasingly deriving data from inherently error-prone processes such as extracting structured data from websites, synthesizing data from multiple networked sensors, and linking entities in disparate data sources.
As these data grow, new methodologies for scalable and reliable analysis in the presence of errors are required. 

Increasingly, modern data analysis pipelines involve Machine Learning for predictive models.
The endpoint of these pipelines can be any number of ``data products", such as recommender systems, spam detectors, and forecasting models, all of which can be very sensitive to data quality \cite{}.
When data error is systematic,i.e., it is correlated with the data, errors can significantly bias predictions by a model.
For example, in a recommender system, we may find that all users from one region have a missing age attribute.
In this setting, discarding corrupted data or ignoring the problem can make predictions for the affected subpopulation untrustworthy.
A more sophisticated approach is to apply robust statistical methods, but these are designed to mitigate the effect of high-magnitude random outliers, and not compensate for systematic errors.

One solution to the systematic error problem is data cleaning, which is an extensively studied field (see Rahm and Do \cite{rahm2000data} for a survey).
Instead of avoiding the problem, cleaning works by repairing (or approximately repairing) the corruption.
However, cleaning large data can be expensive, both computationally and in human effort, as an analyst has to program repairs for all errors manifest in the data \cite{kandel2012}.
In some applications, scripted data transformations may not be reliable necessitating the use of even costlier crowdsourcing techniques \cite{gokhale2014corleone,park2014crowdfill}.

An emerging solution to the growing costs of data cleaning is sampling \cite{wang1999sample} where the analyst cleans a small sample of data and can estimate the results of aggregate queries.Analysts can sample a large dataset, prototype changes on the sample, and evaluate whether these changes have the desired affect.
The case for sampling is analagous to arguments for Approximate Query Processing (AQP) \cite{DBLP:conf/eurosys/AgarwalMPMMS13}, where a timely approximate answer is more desirable than an exact slow answer.
Sampling provides a flexible tradeoff between cleaning cost and query result accuracy for aggregate queries, but the question we explore in this paper is how this tradeoff extends to Machine Learning. 
Machine Learning is far more sensitive to sample size (i.e., training data) than aggregate queries and we explore how we can exploit the geometery of the model to prioritize sampling.

This insight is not surprising, as the growing consensus in Machine Learning research is that all training data are not created equal and some data are more informative than others \cite{drineas2012fast, settles2010active}.
In Active Learning, we often sequentially select the most important points to label \cite{settles2010active}.
However, the key new challenge in data cleaning is that features and examples that may look unimportant in the dirty data may be important in the clean data and vice versa.
We exploit two properties of this problem, sparsity and history, to build a priortization framework that identifies examples that are valuable to the clean model.
First, errors are often happen in batches are often tightly clustered, that is they affect similar data.
Second, as we clean more data, we learn how errors affect the data.
We can use these two insights to partition data that we believe is clean from the dirty data, and estimate the effects of cleaning on the dirty data based on prior examples.
The resulting algorithm selects examples valuable to the clean model with higher probability and as an analyst cleans data we further guide the cleaning.

In this paper, we propose \sysfull (\sys), an anytime framework for training Machine Learning models with data cleaning.
\sys supports a class of models called regularized-convex loss problems which includes linear regression, logistic regression, generalized linear models, and support vector machines.
These models are typically solved with a stochastic optimization technique such as Stochastic Gradient Descent (SGD).
The basic idea of SGD is to draw a data point at random, calculate the gradient at that point, and then update a current best estimate with that gradient.
This stochastic process fits nicely with the idea of sampling and cleaning.
As we draw samples of our training examples, we clean the data as required by the optimization intelligently avoiding data that we have already cleaned or we believe is clean.
Our main insight is that these samples can be drawn with importance sampling, which has recently been studied in stochastic optimization \cite{zhao2014stochastic} but never in the context of online data cleaning. 
The optimization iteratively converges giving us the desired anytime behavior.

In summary, our contributions are
\begin{itemize}[noitemsep]
\item We propose \sysfull which given dirty data, a convex loss model, and a time budget, returns a model trained with respect to the clean data.
\item \sysfull is implemented with an non-uniform importance sampling approach that priortizes points in the clean model that are most imformative, and we provide analysis to identify the key tradeoffs.
\item There are numerous database techniques that we apply to make this practical. We have to index dirty and clean data, amortize scan costs using shared cursors, and modify some data cleaning operations for correctness when applied to progressively increasing sample sizes.
\item We evaluate \sysfull on real and synthetic datasets to show that non-uniform sampling achieves improved performance in comparison to uniform sampling, and dirtiness-agnostic prioritization.
\end{itemize}








