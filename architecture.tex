\section{System Architecture}\label{arch}
We describe the \sys architecture and the basic algorithmic framework.
We will address the individual components in the subsequent sections.

\subsection{Overview}
We revisit the architecture diagram (Figure \ref{sys-arch}) in more detail.
The first step of the system is \emph{initialization}.
In this step, the user provides a relation $R$, a data cleaning technique $C(\cdot)$, and a dirty model $\theta^{(d)}$ trained on the dirty dataset. 
Optionally, the user can provide a dirty data detector $D(\cdot)$ which selects the set of likely corrupted records from $R$.
This is possible for some types of data errors such as constraint violations and missing values.
If one is not provided, we start by treating all of the data is dirty and try to learn a detector as we clean more data.
We are given a \emph{featurization} function $F(\cdot)$ that maps each record in $R$ to a feature vector and a label.
At initialization, there are two hyperparameters to set, the cleaning budget $k$ and the batch size $b$ (the number of iterations is $t = \frac{k}{b}$).
We discuss how to set $b$ and the tradeoffs in setting a larger or smaller $b$ in Section \label{model-update}.

After initialization, \sys begins the cleaning and model update iterations.
The \emph{sampler} selects a sample of dirty data based on the batch size.
At this step, \sys can use the detector $D$ to narrow the sample to select only dirty data.
Once a sample is selected, we apply the user-specified data cleaning operation to clean the sample (the \emph{cleaner}).
Then, after the sample is cleaned, the dirty model is updated by the \emph{updater}.

The next two steps in the archictecture are feedback steps where we update the sampling for the next iteration.
The \emph{estimator} estimates the change on the model if a dirty record is cleaned based on prior cleaned data.
This information can be used to guide sampling towards more valuable records.
After estimation, we also update our detector $D(\cdot)$ based on the data that we have cleaned.
After all of the iterations are complete, the system returns the updated model.

To summarize the architecture in pseudocode:
\begin{enumerate}\scriptsize\sloppy
\item \texttt{Init(dirty\_data, dirty\_model, batch, iter)}
\item For each i in $\{1,...,T\}$
\begin{enumerate}
	\item \texttt{dirty\_sample $=$ Sampler(dirty\_data, sample\_prob, detector, batch)}
	\item \texttt{clean\_sample $=$ Repair(dirty\_sample)}
	\item \texttt{current\_model $=$ Updater(current\_model, sample\_prob, clean\_sample)}
	\item \texttt{sample\_prob $=$ Estimator(dirty\_data, cleaned\_data, detector)}
	\item \texttt{detector $=$ DetectorUpdater(detector)}
\end{enumerate}
\item \texttt{Output: current_model}
\end{enumerate}

\subsection{Challenges and Formalization}
We highlight the important components and formalize the research questions that we explore in this paper. 

\vspace{0.5em}

\noindent \textbf{Featurization. } Given a relation $R$ with a set of attributes $A$.
There is a featurization function $F$ which maps every row in $\mathcal{R}$ to a $d$ dimensional feature vector and a $l$ dimensional label tuple: 
\[F(r \in R) \rightarrow (\mathbb{R}^l, \mathbb{R}^d)\]
The result of the featurization are the data matrices $X$ and $Y$.
\[
F(R)\rightarrow (X,Y)
\]

\vspace{0.5em}

\noindent\textbf{Detector (Section \ref{det}). } The first challenge in \sys is dirty data detection. This is key part of our sampling framework. In this step, we select a candidate set of dirty records $R_{dirty} \subseteq R$. We will discuss two techniques to do this: (1) an a priori case, and (2) and an adaptive case. In the a priori case, we know which data is dirty in advance. In the adaptive case, we train a classifier based on data that we have already cleaned to select the dirty data.

\vspace{0.5em}

\noindent\textbf{Sampler (Section \ref{dist-samp}). } We take a sample of the records $S_{dirty} \subseteq R_{dirty}$. This is a non-uniform sample where each record $r$ has a sampling probability $p_r$.
We will show that we can derive the optimal sampling distribution theoretically, however, it will depend on knowing the cleaned data before we actually clean. This is why we introduce an estimation step to approximate this cleaned value.

\vspace{0.5em}

\noindent\textbf{Updater (Section \ref{model-update}). } Then, we update the model $\theta^({(i)})$ based on the newly cleaned data $F(S_{clean})$. The result is the updated model $\theta^({(i+1)})$. We will present the model update before we derive the optimal sampling distribution.
This is because that distribution will naturally fall out of our update procedure.

\vspace{0.5em}

\noindent\textbf{Estimator (Section \ref{sampling}): } The estimator approximates the optimal distribution derived in the Sample step. Based on the change between $F(S_{clean})$ and $F(S_{dirty})$, we direct the next iteration of sampling to select points that will have changes most valuable to the next model update.

\iffalse
\subsection{Optimizations}
There are three aspects of \sys, that allow us to achieve this design point: error partitioning, gradient-based model update (Section \ref{model-update}), estimate-driven sampling (Section \ref{sampling}).

\vspace{0.5em}

\noindent\textbf{Partitioning Dirty and Clean Data: } In many applications, enumerating the set of corrupted records is much easier than cleaning them. For example, we may be able to select the set of rows that have missing values but actually filling those missing values is expensive. Likewise, in the constraint literature, selecting a set of rows that have a violated constraint can be done in polynomial time, however, fixing the constraints is NP-Hard.
In our error detection step, we partition the dirty and clean data.
Partitioning serves two purposes: (1) it reduces the variance of our updates because we can cheaply scan over data we know that is clean, and (2) it increases the fraction of actually dirty records in the candidate batch.
A good example of why we need the second objective is seen in the context of crowdsourcing.
If we have a crowdworker clean records, we will have to pay them for the task whether or not the record required cleaning.
To efficiently use this partitioning, we need a database solution indexing dirty and clean data.

\vspace{0.5em}

\noindent\textbf{Gradient-Based Updates: } In \sys, we start with a dirty model and then make an update using a gradient step. Here, we can draw an analogy to Materialized View maintenance, since after all, a model parametrized by $\theta$ is just a table of floating point numbers.
Krishnan et al. proposed a technique called sample view cleaning, in which they take a clean sample of data and propagate the updates to a Materialized View.
Similarly, in this work, we take the information from a sample of cleaned data and propagate an update with the gradient.

\vspace{0.5em}

\noindent\textbf{Estimate-Driven Sampling: } Repair is the most expensive step in the workflow, so optimizing for scan cost may lead to negligible overall time improvements.
We can sacrifice a small overhead in pre-computation for each data point to determine its value to the model and select a sampling distribution accordingly.
Intuitively, while each iteration has an increased cost, it also makes more progress towards the optimum.
\fi


