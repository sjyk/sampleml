\section{Classifier Partitioning}\label{imperfect} 
In this section, we explore the case when the partitioning is imperfect.

\subsection{Using a classifier}
An important aspect of our gradient update is partitioning the dirty and clean data.
We aggregate an average gradient from both subpopulations when making our update.
When our partitioning is perfect, at least all of the dirty data is selected in $R_{dirty}$, this estimate in unbiased.
However, in some cases, characterizing this partioning can be difficult and it may be impossible to ensure this condition unless $R_{dirty} = R$ causing a loss in efficiency if errors are sparse. 

We retain the assumption that error detection is easier than repair.
In particular, we argue that it is more feasible to train a classifier to predict error occurance than to train a regression to fix the consequences of error.
Let there be $u$ types of errors in the database.
Suppose, we have a multi-class classifier $\kappa$ (e.g. SVM) that classifies every record $r$ into
$u+1$ classes (the $u$ error types or not dirty).

To make such a classifier useful, we have the modify our problem formulation:

\vspace{0.5em}

\noindent\textbf{Error Detection: } To select $R_{dirty}$, we select the set of records for which $\kappa$ give a positive error classification (i.e., one of the $u$ error classes).
Many types of classifiers allow users to tradeoff precision and recall.
In other words, we can also select any record within some level of confidence of the classification margin.
For an SVM, we may only classify a point as clean if it is sufficiently far from the margin.
Or for Logistic Regression, we may do so if its class likelihood is over 80\%.

\vspace{0.5em}

\noindent\textbf{Error Repair: } When an example $(x,y)$ is cleaned, the repair step also has to provide a label of to which of the $u+1$ classes it belongs. As more data is cleaned, this data becomes training data for the classifier.

\subsection{Error Estimate}
Our model update framework still holds in this setting, however, our error estimation framework requires some modification.
In our derivation, we showed that our error estimate was a linearization of the conditional expectation of the gradient.
Instead of conditioning on the features that are corrupted, we condition the error classes.
So for each error class, we compute a $\Delta_{xu}$ and $\Delta_{yu}$.
These are the average change in the features given that class and the average change in labels given that class respectively.
\[
p_{r,u}\propto\|\nabla\phi(x,y,\theta^{(t)}) + M_x \cdot \Delta_{ux} +  M_y \cdot \Delta_{uy}\|
\] 

